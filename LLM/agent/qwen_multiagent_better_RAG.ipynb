{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation import GenerationConfig\n",
    "import torch\n",
    "\n",
    "# import\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain import LLMChain\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms.base import LLM\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from typing import Any, Dict, List, Mapping, Optional, Tuple, Union\n",
    "from torch.mps import empty_cache\n",
    "import torch\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from typing import Any, List, Optional\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from langchain.llms.base import BaseLLM\n",
    "from typing import List, Optional, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-20 13:03:11.978561: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-20 13:03:12.099278: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-20 13:03:12.696315: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64\n",
      "2024-06-20 13:03:12.696386: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64\n",
      "2024-06-20 13:03:12.696392: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84640afc216040f985d0c77c76fa020f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class QwenRunnable(LLM, BaseModel):\n",
    "    model: Any\n",
    "    tokenizer: Any\n",
    "    device: str = \"cuda:7\"\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        response, _ = self.model.chat(self.tokenizer, query=prompt, history=None)\n",
    "        return response\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"qwen\"\n",
    "\n",
    "class Qwen:\n",
    "    def __init__(self, model_path: str, device: str = \"cuda:7\"):\n",
    "        self.model_path = model_path\n",
    "        self.device = device\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.llm_runnable = None\n",
    "        self.retriever = None\n",
    "\n",
    "    def load_model(self):\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, trust_remote_code=True)\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(self.model_path, trust_remote_code=True)\n",
    "            self.model.to(self.device)\n",
    "            self.model.eval()\n",
    "            self.model.generation_config = GenerationConfig.from_pretrained(self.model_path, trust_remote_code=True)\n",
    "            self.llm_runnable = QwenRunnable(model=self.model, tokenizer=self.tokenizer, device=self.device)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_response(self, prompt: str, history: list = None):\n",
    "        try:\n",
    "            response, history = self.model.chat(self.tokenizer, query=prompt, history=history)\n",
    "            return response, history\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating response: {e}\")\n",
    "            return None, history\n",
    "\n",
    "    def load_retriever(self, doc_path: str, embedding_model_path: str, embedding_device: str = \"cuda:0\"):\n",
    "        try:\n",
    "            # Load documents\n",
    "            loader = TextLoader(doc_path, encoding=\"utf-8\")\n",
    "            documents = loader.load()\n",
    "\n",
    "            # Split documents into chunks\n",
    "            text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "            docs = text_splitter.split_documents(documents)\n",
    "\n",
    "            # Create the embedding function\n",
    "            model_kwargs = {'device': embedding_device}\n",
    "            embedding_function = HuggingFaceEmbeddings(model_name=embedding_model_path, model_kwargs=model_kwargs)\n",
    "\n",
    "            # Load into Chroma\n",
    "            db = Chroma.from_documents(docs, embedding_function)\n",
    "            self.retriever = db.as_retriever()\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading retriever: {e}\")\n",
    "            raise\n",
    "\n",
    "    def run_qa(self, query: str):\n",
    "        try:\n",
    "            qa = RetrievalQA.from_chain_type(llm=self.llm_runnable, chain_type=\"stuff\", retriever=self.retriever)\n",
    "            return qa.run(query)\n",
    "        except Exception as e:\n",
    "            print(f\"Error running QA: {e}\")\n",
    "            return None\n",
    "\n",
    "class TopicGPTWithQwen(Qwen):\n",
    "    def generate_topics(self, documents, example_topics):\n",
    "        history = []\n",
    "        topics = example_topics.copy()\n",
    "        for doc in documents:\n",
    "            prompt = f\"Document: {doc}\\nExample Topics: {example_topics}\\nGenerate a new topic if the document doesn't fit existing topics.\"\n",
    "            response, history = self.generate_response(prompt, history)\n",
    "            if response:\n",
    "                topics.append(response)\n",
    "        return topics\n",
    "\n",
    "    def refine_topics(self, topics):\n",
    "        model = SentenceTransformer('/data1/dxw_data/llm/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "        topic_embeddings = model.encode(topics, convert_to_tensor=True)\n",
    "        refined_topics = []\n",
    "\n",
    "        for i in range(len(topics)):\n",
    "            if topics[i] not in refined_topics:\n",
    "                for j in range(i + 1, len(topics)):\n",
    "                    if util.cos_sim(topic_embeddings[i], topic_embeddings[j]) >= 0.5:\n",
    "                        break\n",
    "                else:\n",
    "                    refined_topics.append(topics[i])\n",
    "\n",
    "        final_topics = []\n",
    "        history = []\n",
    "        for topic in refined_topics:\n",
    "            prompt = f\"Topic: {topic}\\nRefined Topics: {refined_topics}\\nDo you agree this topic should be kept?\"\n",
    "            response, history = self.generate_response(prompt, history)\n",
    "            if response and response.lower() not in [\"no\", \"disagree\"]:\n",
    "                final_topics.append(topic)\n",
    "\n",
    "        return final_topics\n",
    "\n",
    "    def assign_topics(self, documents, topics):\n",
    "        history = []\n",
    "        assignments = {}\n",
    "\n",
    "        for doc in documents:\n",
    "            prompt = f\"Document: {doc}\\nTopics: {topics}\\nAssign the most relevant topic to the document and provide a quote.\"\n",
    "            response, history = self.generate_response(prompt, history)\n",
    "            if response:\n",
    "                assignments[doc] = response\n",
    "\n",
    "        return assignments\n",
    "\n",
    "    def self_correct(self, assignments):\n",
    "        history = []\n",
    "        corrected_assignments = {}\n",
    "\n",
    "        for doc, assignment in assignments.items():\n",
    "            if \"None\" in assignment or \"Error\" in assignment:\n",
    "                prompt = f\"Document: {doc}\\nError: {assignment}\\nPlease reassign a valid topic.\"\n",
    "                response, history = self.generate_response(prompt, history)\n",
    "                if response:\n",
    "                    corrected_assignments[doc] = response\n",
    "            else:\n",
    "                corrected_assignments[doc] = assignment\n",
    "\n",
    "        return corrected_assignments\n",
    "\n",
    "# 使用示例\n",
    "\n",
    "# Path to the model directory\n",
    "model_path = \"/data1/dxw_data/llm/Qwen-VL-Chat\"\n",
    "device = 'cuda:7'\n",
    "\n",
    "# Instantiate and load the model\n",
    "qwen_model = TopicGPTWithQwen(model_path, device)\n",
    "qwen_model.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 127, which is longer than the specified 100\n",
      "Created a chunk of size 119, which is longer than the specified 100\n",
      "Created a chunk of size 101, which is longer than the specified 100\n",
      "Created a chunk of size 229, which is longer than the specified 100\n",
      "Created a chunk of size 105, which is longer than the specified 100\n",
      "Created a chunk of size 152, which is longer than the specified 100\n",
      "Created a chunk of size 101, which is longer than the specified 100\n",
      "Created a chunk of size 115, which is longer than the specified 100\n",
      "Created a chunk of size 182, which is longer than the specified 100\n",
      "Created a chunk of size 104, which is longer than the specified 100\n",
      "Created a chunk of size 185, which is longer than the specified 100\n",
      "Created a chunk of size 199, which is longer than the specified 100\n",
      "Created a chunk of size 104, which is longer than the specified 100\n",
      "Created a chunk of size 404, which is longer than the specified 100\n",
      "No sentence-transformers model found with name /data1/dxw_data/llm/text2vec-large-chinese. Creating a new one with MEAN pooling.\n"
     ]
    }
   ],
   "source": [
    "# 示例数据\n",
    "documents = [\n",
    "    \"The stock market saw a significant increase in value due to positive economic policies.\",\n",
    "    \"New agricultural techniques have improved crop yields significantly.\",\n",
    "    \"Tech companies are investing heavily in artificial intelligence research.\",\n",
    "    \"Economic growth is expected to continue with new trade agreements.\",\n",
    "    \"Farmers are adopting new technologies to boost production.\"\n",
    "]\n",
    "\n",
    "example_topics = [\n",
    "    \"Economy: Mentions policies, growth, and financial markets.\",\n",
    "    \"Agriculture: Discusses farming techniques, crop yields, and agricultural policies.\"\n",
    "]\n",
    "\n",
    "# Load the retriever\n",
    "doc_path = \"/data1/dxw_data/llm/mkt_llm/starbuck/starbuck_comments_1.txt\"\n",
    "embedding_model_path = \"/data1/dxw_data/llm/text2vec-large-chinese\"\n",
    "qwen_model.load_retriever(doc_path, embedding_model_path, embedding_device='cuda:6')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------更加符合论文agentverse\n",
    "\n",
    "# 解释\n",
    "# generate_topics：使用代理处理每个文档，生成新的主题。通过保存和加载 history，每次调用都包含之前的上下文。\n",
    "# refine_topics：使用 SentenceTransformer 模型对主题进行编码，然后通过余弦相似度去除相似的主题。代理逐个处理每个主题，并通过 history 保存上下文来决定最终的主题。\n",
    "# assign_topics：代理逐个处理每个文档，分配最相关的主题，并提供引用。通过 history 保存上下文。\n",
    "# self_correct：检查并修正无效的分配结果。代理逐个处理每个文档，并通过 history 保存上下文来提供修正。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Topics:\n",
      "['Economy: Mentions policies, growth, and financial markets.', 'Agriculture: Discusses farming techniques, crop yields, and agricultural policies.', \"['Investment: Analyzes the impact of positive economic policies on the stock market and financial markets as a whole']\", \"['Agriculture: Examines the benefits of new agricultural techniques on crop yields and their potential impact on the economy and financial markets']\", \"['Tech Industry: Analyzes the potential impact of increased investment in artificial intelligence research on the tech industry and the economy as a whole']\", \"['Trade: Examines the potential benefits and risks of new trade agreements on economic growth and the financial markets']\", \"['Agriculture: Analyzes the impact of new technologies on crop yields and the potential benefits for farmers and the economy as a whole']\"]\n",
      "Refined Topics:\n",
      "[\"['Tech Industry: Analyzes the potential impact of increased investment in artificial intelligence research on the tech industry and the economy as a whole']\", \"['Trade: Examines the potential benefits and risks of new trade agreements on economic growth and the financial markets']\", \"['Agriculture: Analyzes the impact of new technologies on crop yields and the potential benefits for farmers and the economy as a whole']\"]\n",
      "Topic Assignments:\n",
      "Document: The stock market saw a significant increase in value due to positive economic policies.\n",
      "Assignment: The most relevant topic for the given document is \"Trade: Examines the potential benefits and risks of new trade agreements on economic growth and the financial markets\". A quote from the document could be: \"The stock market saw a significant increase in value due to positive economic policies, with trade agreements being a key factor in this growth.\"\n",
      "\n",
      "Document: New agricultural techniques have improved crop yields significantly.\n",
      "Assignment: The most relevant topic for the given document is \"Agriculture: Analyzes the impact of new technologies on crop yields and the potential benefits for farmers and the economy as a whole\". A quote from the document could be: \"The stock market saw a significant increase in value due to positive economic policies, with new agricultural techniques playing a key role in this growth.\"\n",
      "\n",
      "Document: Tech companies are investing heavily in artificial intelligence research.\n",
      "Assignment: The most relevant topic for the given document is \"Tech Industry: Analyzes the potential impact of increased investment in artificial intelligence research on the tech industry and the economy as a whole\". A quote from the document could be: \"The stock market saw a significant increase in value due to positive economic policies, with tech companies investing heavily in artificial intelligence research playing a key role in this growth.\"\n",
      "\n",
      "Document: Economic growth is expected to continue with new trade agreements.\n",
      "Assignment: The most relevant topic for the given document is \"Trade: Examines the potential benefits and risks of new trade agreements on economic growth and the financial markets\". A quote from the document could be: \"The stock market saw a significant increase in value due to positive economic policies, with new trade agreements playing a key role in this growth.\"\n",
      "\n",
      "Document: Farmers are adopting new technologies to boost production.\n",
      "Assignment: The most relevant topic for the given document is \"Agriculture: Analyzes the impact of new technologies on crop yields and the potential benefits for farmers and the economy as a whole\". A quote from the document could be: \"The stock market saw a significant increase in value due to positive economic policies, with farmers adopting new technologies to boost production playing a key role in this growth.\"\n",
      "\n",
      "Corrected Topic Assignments:\n",
      "Document: The stock market saw a significant increase in value due to positive economic policies.\n",
      "Corrected Assignment: The most relevant topic for the given document is \"Trade: Examines the potential benefits and risks of new trade agreements on economic growth and the financial markets\". A quote from the document could be: \"The stock market saw a significant increase in value due to positive economic policies, with trade agreements being a key factor in this growth.\"\n",
      "\n",
      "Document: New agricultural techniques have improved crop yields significantly.\n",
      "Corrected Assignment: The most relevant topic for the given document is \"Agriculture: Analyzes the impact of new technologies on crop yields and the potential benefits for farmers and the economy as a whole\". A quote from the document could be: \"The stock market saw a significant increase in value due to positive economic policies, with new agricultural techniques playing a key role in this growth.\"\n",
      "\n",
      "Document: Tech companies are investing heavily in artificial intelligence research.\n",
      "Corrected Assignment: The most relevant topic for the given document is \"Tech Industry: Analyzes the potential impact of increased investment in artificial intelligence research on the tech industry and the economy as a whole\". A quote from the document could be: \"The stock market saw a significant increase in value due to positive economic policies, with tech companies investing heavily in artificial intelligence research playing a key role in this growth.\"\n",
      "\n",
      "Document: Economic growth is expected to continue with new trade agreements.\n",
      "Corrected Assignment: The most relevant topic for the given document is \"Trade: Examines the potential benefits and risks of new trade agreements on economic growth and the financial markets\". A quote from the document could be: \"The stock market saw a significant increase in value due to positive economic policies, with new trade agreements playing a key role in this growth.\"\n",
      "\n",
      "Document: Farmers are adopting new technologies to boost production.\n",
      "Corrected Assignment: The most relevant topic for the given document is \"Agriculture: Analyzes the impact of new technologies on crop yields and the potential benefits for farmers and the economy as a whole\". A quote from the document could be: \"The stock market saw a significant increase in value due to positive economic policies, with farmers adopting new technologies to boost production playing a key role in this growth.\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dxw/anaconda3/envs/agent/lib/python3.8/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "根据文档内容，用户评论主要分为以下主题：\n",
      "1. 老婆饼：有用户表示御蝶坊的老婆饼是最有名的，味道好，但也有用户表示购买的老婆饼口味发酸，怀疑是之前未卖完的重新装袋打日期卖的。\n",
      "2. 生日蛋糕：有用户表示御蝶坊的生日蛋糕非常受欢迎，性价比高，是经常购买的款式之一。\n",
      "3. 面包：有用户表示御蝶坊的面包是真材实料，买给孩子吃放心，每周都会多次购买作为早餐，且办理了会员卡。\n",
      "4. 蛋挞：有用户表示御蝶坊的蛋挞非常好吃，特别是海盐蛋糕和肉松小贝，香芋那个也很好吃。\n",
      "5. 价格：有用户表示御蝶坊的产品价格越来越高，性价比有所下降。\n",
      "6. 味道：有用户表示御蝶坊的产品味道一直不错，特别是老婆饼和蛋挞，但也有用户表示价格越来越高，感觉不值。\n",
      "7. 服务：有用户表示御蝶坊的服务态度很好，卫生环境也很好。\n"
     ]
    }
   ],
   "source": [
    "# Generate topics\n",
    "generated_topics = qwen_model.generate_topics(documents, example_topics)\n",
    "print(\"Generated Topics:\")\n",
    "print(generated_topics)\n",
    "\n",
    "# Refine topics\n",
    "refined_topics = qwen_model.refine_topics(generated_topics)\n",
    "print(\"Refined Topics:\")\n",
    "print(refined_topics)\n",
    "\n",
    "# Assign topics\n",
    "assignments = qwen_model.assign_topics(documents, refined_topics)\n",
    "print(\"Topic Assignments:\")\n",
    "for doc, assignment in assignments.items():\n",
    "    print(f\"Document: {doc}\\nAssignment: {assignment}\\n\")\n",
    "\n",
    "# Self-correct\n",
    "corrected_assignments = qwen_model.self_correct(assignments)\n",
    "print(\"Corrected Topic Assignments:\")\n",
    "for doc, assignment in corrected_assignments.items():\n",
    "    print(f\"Document: {doc}\\nCorrected Assignment: {assignment}\\n\")\n",
    "\n",
    "# Example QA usage\n",
    "query = \"根据文档内容,请说明有哪些这些用户评论分为哪些主题\"\n",
    "qa_response = qwen_model.run_qa(query)\n",
    "print(qa_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
