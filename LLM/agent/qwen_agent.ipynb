{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fce199f2710>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation import GenerationConfig\n",
    "import torch\n",
    "\n",
    "# import\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain import LLMChain\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms.base import LLM\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from typing import Any, Dict, List, Mapping, Optional, Tuple, Union\n",
    "from torch.mps import empty_cache\n",
    "import torch\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "torch.manual_seed(1234)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-15 09:03:13.678505: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-15 09:03:13.797971: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-15 09:03:14.367473: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64\n",
      "2024-06-15 09:03:14.367746: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64\n",
      "2024-06-15 09:03:14.367754: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9127356e41d54dd18bda808d906096c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "class Qwen:\n",
    "    def __init__(self, model_path: str, device: str = \"cuda:0\"):\n",
    "        self.model_path = model_path\n",
    "        self.device = device\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "\n",
    "    def load_model(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, trust_remote_code=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(self.model_path, trust_remote_code=True)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        self.model.generation_config = GenerationConfig.from_pretrained(self.model_path, trust_remote_code=True)\n",
    "\n",
    "    def generate_response(self, image_folder: str, image_index: int, prompt: str):\n",
    "        query = self.tokenizer.from_list_format([\n",
    "            {'image': f'{image_folder}/{image_index}.png'}, \n",
    "            {'text': prompt}\n",
    "        ])\n",
    "\n",
    "        # Ensure tensors are on the correct device if necessary\n",
    "        if isinstance(query, torch.Tensor):\n",
    "            query = query.to(self.device)\n",
    "\n",
    "        response, history = self.model.chat(self.tokenizer, query=query, history=None)\n",
    "        return response\n",
    "\n",
    "# Path to the model directory\n",
    "model_path = \"/data1/dxw_data/llm/Qwen-VL-Chat\"\n",
    "# Specify the device (e.g., 'cuda:0', 'cuda:1')\n",
    "device = 'cuda:0'\n",
    "\n",
    "# Instantiate and load the model\n",
    "qwen_model = Qwen(model_path, device)\n",
    "qwen_model.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The woman in the image is wearing a beautiful pastel pink skirt that flows beautifully when she stands on the stairs. She also wears a simple white top that complements the skirt perfectly. The outfit is completed with a pair of comfortable beige sandals. The overall style is very suitable for a casual summer outing with friends.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "image_folder = \"/data1/dxw_data/llm/mkt-englishwords/data\"\n",
    "image_index = 0  # Change this to the actual image index\n",
    "prompt = \"Generate some descriptions of the clothes in this image, following the social media style of tone\"\n",
    "\n",
    "response = qwen_model.generate_response(image_folder, image_index, prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
