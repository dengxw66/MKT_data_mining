{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取关键词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags have been successfully saved to /data1/dxw_data/llm/redbook_final/data/red_01/tags.txt.\n"
     ]
    }
   ],
   "source": [
    "# 第1步\n",
    "import pandas as pd\n",
    "\n",
    "# 读取CSV文件\n",
    "file_path = '/data1/dxw_data/llm/redbook_final/data/red_01/merged_good_raw.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 获取post_tag列\n",
    "post_tags = df['post_tag']\n",
    "\n",
    "# 初始化一个空集合，用于存储所有标签\n",
    "all_tags = set()\n",
    "\n",
    "# 遍历post_tag列，将标签分割并加入集合\n",
    "for tags in post_tags:\n",
    "    if isinstance(tags, str):\n",
    "        tags_list = tags.split()\n",
    "        all_tags.update(tags_list)\n",
    "\n",
    "# 将集合转换为列表，并按字典序排序\n",
    "sorted_tags = sorted(all_tags)\n",
    "\n",
    "# 保存到txt文件\n",
    "output_file = '/data1/dxw_data/llm/redbook_final/data/red_01/tags.txt'\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for tag in sorted_tags:\n",
    "        f.write(f\"{tag}\\n\")\n",
    "\n",
    "print(f\"Tags have been successfully saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results have been successfully saved to /data1/dxw_data/llm/redbook_final/data/red_01/matched_categories.csv.\n"
     ]
    }
   ],
   "source": [
    "# 第2步\n",
    "import pandas as pd\n",
    "\n",
    "# 读取CSV文件\n",
    "file_path = '/data1/dxw_data/llm/redbook_final/data/red_01/merged_good_raw.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 定义关键词列表\n",
    "key_words = [\n",
    "    \"裙\", \"裙子\", \"项链\", \"配饰\", \"裤\", \"吊带\", \"风格\", \"饰品\", \"单品\", \"衬衫\", \"身材\", \"耳环\", \"主义\", \"混搭\", \n",
    "    \"手链\", \"元素\", \"绒\", \"肩\", \"鞋子\", \"瘦\", \"套装\", \"款\", \"毛\", \"吊坠\", \"造型\", \"型\", \"饰\", \"袜\", \n",
    "    \"马甲\", \"系\", \"夹克\", \"裳\", \"推荐\", \"服\", \"衣服\", \"靴\", \"款\", \"白t\", \"搭配\", \"恤\", \"大衣\", \"头\", \"风\", \n",
    "    \"毛衣\", \"服\", \"内搭\", \"靴子\", \"链\", \"套装\", \"头发\", \"背心\", \"毛衣\", \"外套\", \"帽\", \"发型\", \"包\", \"衣\", \n",
    "    \"戒指\", \"鞋\", \"衫\", \"袍\", \"手镯\", \"单品\", \"装\", \"镜\", \"帽子\", \"袖\"\n",
    "]\n",
    "\n",
    "# 定义要处理的列\n",
    "columns_to_process = ['post_content', 'post_tag', 'post_comments']\n",
    "\n",
    "# 初始化结果列表\n",
    "results = []\n",
    "\n",
    "# 遍历每一行\n",
    "for _, row in df.iterrows():\n",
    "    categories = set()  # 用集合来去重\n",
    "    for column in columns_to_process:\n",
    "        content = str(row[column])\n",
    "        for word in key_words:\n",
    "            index = content.find(word)\n",
    "            if index != -1:\n",
    "                start_index = max(0, index - 4)\n",
    "                snippet = content[start_index:index]\n",
    "                # 截停在遇到逗号或井号时\n",
    "                if ',' in snippet:\n",
    "                    snippet = snippet.split(',')[-1]\n",
    "                if '#' in snippet:\n",
    "                    snippet = snippet.split('#')[-1]\n",
    "                match_snippet = snippet + content[index:index+len(word)]\n",
    "                categories.add(match_snippet)\n",
    "    \n",
    "    # 仅当categories非空时才保存该行\n",
    "    if categories:\n",
    "        results.append({\n",
    "            'poster_id': row['poster_id'],\n",
    "            'post_id': row['post_id'],\n",
    "            'categories': ', '.join(categories)\n",
    "        })\n",
    "\n",
    "# 转换结果为DataFrame\n",
    "output_df = pd.DataFrame(results)\n",
    "\n",
    "# 保存到新的CSV文件\n",
    "output_file_path = '/data1/dxw_data/llm/redbook_final/data/red_01/matched_categories.csv'\n",
    "output_df.to_csv(output_file_path, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Results have been successfully saved to {output_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word frequencies have been successfully saved to /data1/dxw_data/llm/redbook_final/data/red_01/word_frequencies.txt.\n"
     ]
    }
   ],
   "source": [
    "# 第3步\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# 读取CSV文件\n",
    "file_path = '/data1/dxw_data/llm/redbook_final/data/red_01/matched_categories.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 初始化Counter对象\n",
    "word_counter = Counter()\n",
    "\n",
    "# 处理categories列\n",
    "for categories in df['categories']:\n",
    "    # 分割字符串，去除多余的空格\n",
    "    words = [word.strip() for word in categories.split(',')]\n",
    "    # 更新计数器\n",
    "    word_counter.update(words)\n",
    "\n",
    "# 按照出现次数从高到低排序\n",
    "sorted_words = word_counter.most_common()\n",
    "\n",
    "# 输出到txt文件\n",
    "output_file = '/data1/dxw_data/llm/redbook_final/data/red_01/word_frequencies.txt'\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for word, count in sorted_words:\n",
    "        f.write(f\"{word}: {count}\\n\")\n",
    "\n",
    "print(f\"Word frequencies have been successfully saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始聚类了，下面两步是文本聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting DBSCAN clustering...\n",
      "Processing cluster results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16464/16464 [00:00<00:00, 2411812.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering results have been successfully saved to /data1/dxw_data/llm/redbook_final/data/red_01/clusters_adaptive.json.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 第1步\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# Ensure the device is set to GPU\n",
    "device = torch.device('cuda:5' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Read word frequencies from file\n",
    "word_frequencies_file = '/data1/dxw_data/llm/redbook_final/data/red_01/word_frequencies.txt'\n",
    "elements = []\n",
    "frequencies = []\n",
    "\n",
    "with open(word_frequencies_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().rsplit(': ', 1)\n",
    "        if len(parts) == 2:\n",
    "            element, frequency = parts\n",
    "            elements.append(element)\n",
    "            frequencies.append(int(frequency))\n",
    "\n",
    "# Load model and move it to GPU\n",
    "model = SentenceTransformer('/data1/dxw_data/llm/paraphrase-multilingual-MiniLM-L12-v2', device=device)\n",
    "embeddings = model.encode(elements, convert_to_tensor=True, device=device)\n",
    "\n",
    "# Move embeddings back to CPU for clustering\n",
    "embeddings = embeddings.cpu().numpy()\n",
    "\n",
    "# Use frequencies as weights for DBSCAN clustering\n",
    "weights = np.array(frequencies)\n",
    "eps = 1.0  # Increase the maximum distance between two samples for one to be considered as in the neighborhood of the other\n",
    "min_samples = 40  # Decrease the number of samples in a neighborhood for a point to be considered as a core point\n",
    "\n",
    "# Fit DBSCAN clustering\n",
    "print(\"Fitting DBSCAN clustering...\")\n",
    "db = DBSCAN(eps=eps, min_samples=min_samples).fit(embeddings, sample_weight=weights)\n",
    "\n",
    "# Construct clustering result\n",
    "clusters = {}\n",
    "labels = db.labels_\n",
    "unique_labels = set(labels)\n",
    "\n",
    "for label in unique_labels:\n",
    "    if label != -1:  # Exclude noise points\n",
    "        clusters[int(label)] = []\n",
    "\n",
    "# Display result processing progress\n",
    "print(\"Processing cluster results...\")\n",
    "for idx, label in tqdm(enumerate(labels), total=len(labels)):\n",
    "    if label != -1:  # Exclude noise points\n",
    "        clusters[int(label)].append(elements[idx])\n",
    "\n",
    "# Output result to JSON file\n",
    "output_file = '/data1/dxw_data/llm/redbook_final/data/red_01/clusters_adaptive.json'\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(clusters, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Clustering results have been successfully saved to {output_file}.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8638/8638 [00:00<00:00, 14370.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CSV file has been successfully saved to /data1/dxw_data/llm/redbook_final/data/red_01/matched_categories_with_clusters_adaptive.csv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 第2步\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 读取clusters.json文件\n",
    "clusters_file = '/data1/dxw_data/llm/redbook_final/data/red_01/clusters_adaptive.json'\n",
    "with open(clusters_file, 'r', encoding='utf-8') as f:\n",
    "    clusters = json.load(f)\n",
    "\n",
    "# 创建关键词到类别的映射\n",
    "keyword_to_cluster = {}\n",
    "for cluster_id, keywords in clusters.items():\n",
    "    for keyword in keywords:\n",
    "        keyword_to_cluster[keyword.strip()] = cluster_id\n",
    "\n",
    "# 读取matched_categories.csv文件\n",
    "matched_categories_file = '/data1/dxw_data/llm/redbook_final/data/red_01/matched_categories.csv'\n",
    "df = pd.read_csv(matched_categories_file)\n",
    "\n",
    "# 初始化num_category列\n",
    "df['num_category_text'] = ''\n",
    "\n",
    "# 处理每一行\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    categories = row['categories']\n",
    "    matched_clusters = set()\n",
    "    for category in categories.split(','):\n",
    "        category = category.strip()\n",
    "        if category in keyword_to_cluster:\n",
    "            matched_clusters.add(keyword_to_cluster[category])\n",
    "    df.at[idx, 'num_category_text'] = ', '.join(matched_clusters)\n",
    "\n",
    "# 保存更新后的CSV文件\n",
    "output_file = '/data1/dxw_data/llm/redbook_final/data/red_01/matched_categories_with_clusters_adaptive.csv'\n",
    "df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Updated CSV file has been successfully saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 图片聚类\n",
    "\n",
    "# 在这里之前，先要去imagebind文件夹，运行imagebind聚类算法。\n",
    "# /data1/dxw_data/llm/ImageBind/0cluster.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully saved to /data1/dxw_data/llm/redbook_final/data/clustered_labels_adaptive.csv.\n"
     ]
    }
   ],
   "source": [
    "# 第1步，格式转化\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# 读取labels.json文件\n",
    "labels_file = '/data1/dxw_data/llm/redbook_final/data/output_cluster_imagebind_adaptive/labels.json'\n",
    "with open(labels_file, 'r', encoding='utf-8') as f:\n",
    "    labels_data = json.load(f)\n",
    "\n",
    "# 初始化结果列表\n",
    "results = []\n",
    "\n",
    "# 处理每个标签\n",
    "for image_name, category in labels_data.items():\n",
    "    parts = image_name.split('_')\n",
    "    poster_id = parts[1]\n",
    "    post_id = parts[2]\n",
    "    results.append({\n",
    "        'poster_id': poster_id,\n",
    "        'post_id': post_id,\n",
    "        'nums_category_img': category\n",
    "    })\n",
    "\n",
    "# 转换结果为DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# 保存到新的CSV文件\n",
    "output_file = '/data1/dxw_data/llm/redbook_final/data/clustered_labels_adaptive.csv'\n",
    "df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Data has been successfully saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 图片和文本的合并。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully saved to /data1/dxw_data/llm/redbook_final/data/combined_clustered_matched_image_text_adaptive.csv.\n"
     ]
    }
   ],
   "source": [
    "# 第1步，具体合并\n",
    "import pandas as pd\n",
    "\n",
    "# 读取matched_categories_with_clusters.csv文件\n",
    "matched_categories_file = '/data1/dxw_data/llm/redbook_final/data/red_01/matched_categories_with_clusters_adaptive.csv'\n",
    "matched_df = pd.read_csv(matched_categories_file)\n",
    "\n",
    "# 读取clustered_poster_post_ids.csv文件\n",
    "clustered_poster_post_ids_file = '/data1/dxw_data/llm/redbook_final/data/clustered_labels_adaptive.csv'\n",
    "clustered_df = pd.read_csv(clustered_poster_post_ids_file)\n",
    "\n",
    "# 合并两个数据框\n",
    "merged_df = pd.merge(clustered_df, matched_df, on=['poster_id', 'post_id'], how='left')\n",
    "\n",
    "# 保存结果到新的CSV文件\n",
    "output_file = '/data1/dxw_data/llm/redbook_final/data/combined_clustered_matched_image_text_adaptive.csv'\n",
    "merged_df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Data has been successfully saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category ratios have been successfully saved to /data1/dxw_data/llm/redbook_final/data/category_ratios_adaptive.csv.\n"
     ]
    }
   ],
   "source": [
    "# 第2步：统计指标和参数\n",
    "import pandas as pd\n",
    "\n",
    "# 读取combined_clustered_matched_image_text2.csv文件\n",
    "combined_file = '/data1/dxw_data/llm/redbook_final/data/combined_clustered_matched_image_text_adaptive.csv'\n",
    "df = pd.read_csv(combined_file)\n",
    "\n",
    "# 处理num_category_text列，将其展开成独立的数字类别\n",
    "df['num_category_text'] = df['num_category_text'].fillna('')\n",
    "df['num_category_text'] = df['num_category_text'].apply(lambda x: x.split(', ') if x else [])\n",
    "\n",
    "# 初始化一个字典，用于存储每个nums_category_img对应的num_category_text的计数\n",
    "category_counts = {}\n",
    "\n",
    "# 遍历每一行，统计num_category_text的计数\n",
    "for idx, row in df.iterrows():\n",
    "    nums_category_img = row['nums_category_img']\n",
    "    num_category_texts = row['num_category_text']\n",
    "    \n",
    "    if nums_category_img not in category_counts:\n",
    "        category_counts[nums_category_img] = {}\n",
    "    \n",
    "    for category_text in num_category_texts:\n",
    "        if category_text not in category_counts[nums_category_img]:\n",
    "            category_counts[nums_category_img][category_text] = 0\n",
    "        category_counts[nums_category_img][category_text] += 1\n",
    "\n",
    "# 计算每个nums_category_img中，num_category_text的占据比例\n",
    "category_ratios = {}\n",
    "for nums_category_img, counts in category_counts.items():\n",
    "    total_count = sum(counts.values())\n",
    "    category_ratios[nums_category_img] = {k: v / total_count for k, v in counts.items()}\n",
    "\n",
    "# 转换结果为DataFrame\n",
    "ratios_list = []\n",
    "for nums_category_img, ratios in category_ratios.items():\n",
    "    for num_category_text, ratio in ratios.items():\n",
    "        ratios_list.append({\n",
    "            'nums_category_img': nums_category_img,\n",
    "            'num_category_text': num_category_text,\n",
    "            'ratio': ratio\n",
    "        })\n",
    "\n",
    "ratios_df = pd.DataFrame(ratios_list)\n",
    "\n",
    "# 保存结果到新的CSV文件\n",
    "output_file = '/data1/dxw_data/llm/redbook_final/data/category_ratios_adaptive.csv'\n",
    "ratios_df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Category ratios have been successfully saved to {output_file}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
