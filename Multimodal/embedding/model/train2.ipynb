{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用四舍五入的数值做label，小数点后2位，最后回归拟合的也是msr等值-离散程度，不是准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1638745/756581721.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_after2monthdata['proportion'] = train_after2monthdata['proportion'].round(2)\n",
      "/tmp/ipykernel_1638745/756581721.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_after2monthdata['proportion'] = test_after2monthdata['proportion'].round(2)\n",
      "Processing data: 100%|██████████| 3467/3467 [03:15<00:00, 17.73it/s]\n",
      "Processing data: 100%|██████████| 802/802 [00:45<00:00, 17.81it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from dateutil import parser\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Define file paths\n",
    "data_dir = \"/data1/dxw_data/llm/redbook_final/script_next/\"\n",
    "rawdata_path = os.path.join(data_dir, \"rawdata_20%.csv\")\n",
    "after2monthdata_path = os.path.join(data_dir, \"after2monthdata_20%.csv\")\n",
    "image_dir = os.path.join(data_dir, \"data_img_20%\")\n",
    "\n",
    "# Read CSV files\n",
    "rawdata = pd.read_csv(rawdata_path)\n",
    "after2monthdata = pd.read_csv(after2monthdata_path)\n",
    "\n",
    "# Convert date columns to standard format\n",
    "def parse_date(date_str):\n",
    "    try:\n",
    "        return parser.parse(date_str)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "rawdata['post_date'] = rawdata['post_date'].apply(parse_date)\n",
    "rawdata = rawdata.dropna(subset=['post_date'])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_rawdata = rawdata[(rawdata['post_date'].dt.month >= 1) & (rawdata['post_date'].dt.month <= 9)]\n",
    "test_rawdata = rawdata[(rawdata['post_date'].dt.month >= 10) & (rawdata['post_date'].dt.month <= 12)]\n",
    "\n",
    "train_after2monthdata = after2monthdata[after2monthdata['post_id'].isin(train_rawdata['post_id'])]\n",
    "test_after2monthdata = after2monthdata[after2monthdata['post_id'].isin(test_rawdata['post_id'])]\n",
    "\n",
    "# Round the proportion values to 2 decimal places\n",
    "train_after2monthdata['proportion'] = train_after2monthdata['proportion'].round(2)\n",
    "test_after2monthdata['proportion'] = test_after2monthdata['proportion'].round(2)\n",
    "\n",
    "# Clean the labels\n",
    "train_after2monthdata = train_after2monthdata.replace([np.inf, -np.inf], np.nan).dropna(subset=['proportion'])\n",
    "test_after2monthdata = test_after2monthdata.replace([np.inf, -np.inf], np.nan).dropna(subset=['proportion'])\n",
    "\n",
    "# Only use 1/10th of the data\n",
    "def get_subset_indices(data, fraction=0.1):\n",
    "    data_size = len(data)\n",
    "    indices = list(range(data_size))\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(np.floor(fraction * data_size))\n",
    "    return indices[:split]\n",
    "\n",
    "train_indices = get_subset_indices(train_rawdata)\n",
    "test_indices = get_subset_indices(test_rawdata)\n",
    "\n",
    "train_rawdata = train_rawdata.iloc[train_indices]\n",
    "test_rawdata = test_rawdata.iloc[test_indices]\n",
    "\n",
    "train_after2monthdata = train_after2monthdata[train_after2monthdata['post_id'].isin(train_rawdata['post_id'])]\n",
    "test_after2monthdata = test_after2monthdata[test_after2monthdata['post_id'].isin(test_rawdata['post_id'])]\n",
    "\n",
    "# Custom Dataset Class\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, rawdata, after2monthdata, image_dir, transform=None, max_images=1):\n",
    "        self.rawdata = rawdata\n",
    "        self.after2monthdata = after2monthdata\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.max_images = max_images\n",
    "        self.data = self._prepare_data()\n",
    "    \n",
    "    def _prepare_data(self):\n",
    "        data = []\n",
    "        for _, row in tqdm(self.rawdata.iterrows(), total=self.rawdata.shape[0], desc=\"Processing data\"):\n",
    "            poster_id = row['poster_id']\n",
    "            post_id = row['post_id']\n",
    "            image_files = [f for f in os.listdir(self.image_dir) if f\"{poster_id}_{post_id}\" in f]\n",
    "            images = []\n",
    "            for image_file in image_files[:self.max_images]:\n",
    "                image_path = os.path.join(self.image_dir, image_file)\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "                if self.transform:\n",
    "                    image = self.transform(image)\n",
    "                images.append(image)\n",
    "            # If less than max_images, pad with zeros\n",
    "            while len(images) < self.max_images:\n",
    "                images.append(torch.zeros((3, 224, 224)))\n",
    "            if images:\n",
    "                summary = row['summary']\n",
    "                label_data = self.after2monthdata[self.after2monthdata['post_id'] == post_id]['proportion']\n",
    "                if not label_data.empty:\n",
    "                    label = label_data.values[0]\n",
    "                    data.append((summary, images, label))\n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        summary, images, label = self.data[idx]\n",
    "        return summary, torch.stack(images).float(), torch.tensor(label).float()\n",
    "\n",
    "# Image Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create Datasets\n",
    "train_dataset = MultimodalDataset(train_rawdata, train_after2monthdata, image_dir, transform=transform)\n",
    "test_dataset = MultimodalDataset(test_rawdata, test_after2monthdata, image_dir, transform=transform)\n",
    "\n",
    "# Create Data Loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/10: 100%|██████████| 96/96 [00:47<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.06847658870674422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/10: 100%|██████████| 96/96 [00:48<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 0.0005873890116466404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/10: 100%|██████████| 96/96 [00:48<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 0.0005854149333875588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/10: 100%|██████████| 96/96 [00:48<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 0.0005911607766696155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/10: 100%|██████████| 96/96 [00:48<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 0.0005790043707444662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/10: 100%|██████████| 96/96 [00:48<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 0.0005807790165211676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/10: 100%|██████████| 96/96 [00:47<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 0.0005931772725489282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/10: 100%|██████████| 96/96 [00:48<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 0.0005708792235357881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9/10: 100%|██████████| 96/96 [00:48<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 0.0005663760195299498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10/10: 100%|██████████| 96/96 [00:48<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 0.0005702972586429192\n",
      "Model training completed and saved!\n"
     ]
    }
   ],
   "source": [
    "# Model Definition\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from imagebind import data\n",
    "from imagebind.models import imagebind_model\n",
    "from imagebind.models.imagebind_model import ModalityType\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load imagebind model\n",
    "imagebind_model = imagebind_model.imagebind_huge(pretrained=True)\n",
    "imagebind_model.eval()\n",
    "imagebind_model.to(device)\n",
    "\n",
    "class CrossAttentionFusion(nn.Module):\n",
    "    def __init__(self, text_embedding_dim, vision_embedding_dim, common_embedding_dim, num_heads):\n",
    "        super(CrossAttentionFusion, self).__init__()\n",
    "        self.text_linear = nn.Linear(text_embedding_dim, common_embedding_dim)\n",
    "        self.vision_linear = nn.Linear(vision_embedding_dim, common_embedding_dim)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=common_embedding_dim, nhead=num_heads), num_layers=2)\n",
    "        self.lstm = nn.LSTM(common_embedding_dim, common_embedding_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(common_embedding_dim, 1)\n",
    "    \n",
    "    def forward(self, text_embeddings, vision_embeddings):\n",
    "        text_embeddings = self.text_linear(text_embeddings)\n",
    "        vision_embeddings = self.vision_linear(vision_embeddings)\n",
    "        # Flatten embeddings to match Transformer input requirements\n",
    "        text_embeddings = text_embeddings.view(text_embeddings.size(0), -1, text_embeddings.size(-1))\n",
    "        vision_embeddings = vision_embeddings.view(vision_embeddings.size(0), -1, vision_embeddings.size(-1))\n",
    "        multimodal_embeddings = torch.cat((text_embeddings, vision_embeddings), dim=1)\n",
    "        multimodal_embeddings = self.transformer_encoder(multimodal_embeddings)\n",
    "        lstm_out, _ = self.lstm(multimodal_embeddings)\n",
    "        lstm_out = lstm_out[:, -1, :]  # Take output of the last time step\n",
    "        output = self.fc(lstm_out)\n",
    "        return output\n",
    "\n",
    "def get_embeddings(text_list, image_tensors):\n",
    "    inputs = {\n",
    "        ModalityType.TEXT: data.load_and_transform_text(text_list, device),\n",
    "        ModalityType.VISION: image_tensors.to(device)\n",
    "    }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embeddings = imagebind_model(inputs)\n",
    "    \n",
    "    # Check for NaNs and Infinities in embeddings\n",
    "    if torch.isnan(embeddings[ModalityType.TEXT]).any() or torch.isinf(embeddings[ModalityType.TEXT]).any():\n",
    "        print(\"NaN or Infinity values found in text embeddings\")\n",
    "    if torch.isnan(embeddings[ModalityType.VISION]).any() or torch.isinf(embeddings[ModalityType.VISION]).any():\n",
    "        print(\"NaN or Infinity values found in vision embeddings\")\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    log_file = open(\"log.txt\", \"w\")\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n",
    "            summaries, images, labels = batch\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            text_embeddings = get_embeddings(summaries, images)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(text_embeddings[ModalityType.TEXT], text_embeddings[ModalityType.VISION])\n",
    "            loss = criterion(outputs, labels.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        log_file.write(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss}\\n')\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss}')\n",
    "    log_file.close()\n",
    "\n",
    "# Initialize model, criterion, and optimizer\n",
    "text_embedding_dim = 1024\n",
    "vision_embedding_dim = 1024\n",
    "common_embedding_dim = 768\n",
    "num_heads = 8\n",
    "\n",
    "model = CrossAttentionFusion(text_embedding_dim, vision_embedding_dim, common_embedding_dim, num_heads).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), os.path.join(data_dir, \"multimodal_model.pth\"))\n",
    "\n",
    "print(\"Model training completed and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 25/25 [00:11<00:00,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0005750005086883902\n",
      "Mean Squared Error: 0.000559596810489893\n",
      "Mean Absolute Error: 0.01885504089295864\n",
      "R2 Score: -0.007188295743023154\n",
      "Model testing completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Testing the model\n",
    "def test_model(model, test_loader, criterion):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for summaries, images, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Check for NaNs or Infinities in input data\n",
    "            if torch.isnan(images).any() or torch.isinf(images).any():\n",
    "                print(\"NaN or Infinity values found in input images\")\n",
    "                continue\n",
    "            if torch.isnan(labels).any() or torch.isinf(labels).any():\n",
    "                print(\"NaN or Infinity values found in input labels\")\n",
    "                continue\n",
    "\n",
    "            text_embeddings = get_embeddings(summaries, images)\n",
    "            outputs = model(text_embeddings[ModalityType.TEXT], text_embeddings[ModalityType.VISION])\n",
    "            \n",
    "            # Check for NaNs and Infinities in outputs\n",
    "            if torch.isnan(outputs).any() or torch.isinf(outputs).any():\n",
    "                print(\"NaN or Infinity values found in outputs\")\n",
    "                continue\n",
    "            \n",
    "            loss = criterion(outputs, labels.unsqueeze(1))\n",
    "            if torch.isnan(loss).any() or torch.isinf(loss).any():\n",
    "                print(\"NaN or Infinity values found in loss\")\n",
    "                continue\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            all_predictions.extend(outputs.cpu().numpy().flatten())\n",
    "            all_labels.extend(labels.cpu().numpy().flatten())\n",
    "    \n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    # Check for NaNs and Infinities in all_predictions and all_labels\n",
    "    mask = ~np.isnan(all_predictions) & ~np.isinf(all_predictions) & ~np.isnan(all_labels) & ~np.isinf(all_labels)\n",
    "    all_predictions = all_predictions[mask]\n",
    "    all_labels = all_labels[mask]\n",
    "\n",
    "    mse = mean_squared_error(all_labels, all_predictions)\n",
    "    mae = mean_absolute_error(all_labels, all_predictions)\n",
    "    r2 = r2_score(all_labels, all_predictions)\n",
    "    \n",
    "    print(f'Test Loss: {avg_test_loss}')\n",
    "    print(f'Mean Squared Error: {mse}')\n",
    "    print(f'Mean Absolute Error: {mae}')\n",
    "    print(f'R2 Score: {r2}')\n",
    "    \n",
    "    return avg_test_loss, mse, mae, r2\n",
    "\n",
    "# Initialize model, criterion\n",
    "model = CrossAttentionFusion(text_embedding_dim, vision_embedding_dim, common_embedding_dim, num_heads).to(device)\n",
    "model.load_state_dict(torch.load(os.path.join(data_dir, \"multimodal_model.pth\")))\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Test the model\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "avg_test_loss, mse, mae, r2 = test_model(model, test_loader, criterion)\n",
    "\n",
    "print(\"Model testing completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
