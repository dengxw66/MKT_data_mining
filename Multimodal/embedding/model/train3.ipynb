{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用label变成-1，0，1三分类任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data: 100%|██████████| 346/346 [00:19<00:00, 17.36it/s]\n",
      "Processing data: 100%|██████████| 80/80 [00:04<00:00, 17.02it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from dateutil import parser\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Ensure CUDA_LAUNCH_BLOCKING is set for better debugging\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "# Define file paths\n",
    "data_dir = \"/data1/dxw_data/llm/redbook_final/script_next/\"\n",
    "rawdata_path = os.path.join(data_dir, \"rawdata_20%.csv\")\n",
    "after2monthdata_path = os.path.join(data_dir, \"after2monthdata_20%_with_trend.csv\")\n",
    "image_dir = os.path.join(data_dir, \"data_img_20%\")\n",
    "\n",
    "# Read CSV files\n",
    "rawdata = pd.read_csv(rawdata_path)\n",
    "after2monthdata = pd.read_csv(after2monthdata_path)\n",
    "\n",
    "# Convert date columns to standard format\n",
    "def parse_date(date_str):\n",
    "    try:\n",
    "        return parser.parse(date_str)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "rawdata['post_date'] = rawdata['post_date'].apply(parse_date)\n",
    "rawdata = rawdata.dropna(subset=['post_date'])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_rawdata = rawdata[(rawdata['post_date'].dt.month >= 1) & (rawdata['post_date'].dt.month <= 9)]\n",
    "test_rawdata = rawdata[(rawdata['post_date'].dt.month >= 10) & (rawdata['post_date'].dt.month <= 12)]\n",
    "\n",
    "train_after2monthdata = after2monthdata[after2monthdata['post_id'].isin(train_rawdata['post_id'])]\n",
    "test_after2monthdata = after2monthdata[after2monthdata['post_id'].isin(test_rawdata['post_id'])]\n",
    "\n",
    "# Remove non-finite values in the 'trend' column\n",
    "train_after2monthdata = train_after2monthdata.replace([np.inf, -np.inf], np.nan).dropna(subset=['trend'])\n",
    "test_after2monthdata = test_after2monthdata.replace([np.inf, -np.inf], np.nan).dropna(subset=['trend'])\n",
    "\n",
    "# Ensure that the 'trend' column has correct integer labels\n",
    "train_after2monthdata['trend'] = train_after2monthdata['trend'].astype(int)\n",
    "test_after2monthdata['trend'] = test_after2monthdata['trend'].astype(int)\n",
    "\n",
    "# Only use 1/10th of the data\n",
    "def get_subset_indices(data, fraction=0.01):\n",
    "    data_size = len(data)\n",
    "    indices = list(range(data_size))\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(np.floor(fraction * data_size))\n",
    "    return indices[:split]\n",
    "\n",
    "train_indices = get_subset_indices(train_rawdata)\n",
    "test_indices = get_subset_indices(test_rawdata)\n",
    "\n",
    "train_rawdata = train_rawdata.iloc[train_indices]\n",
    "test_rawdata = test_rawdata.iloc[test_indices]\n",
    "\n",
    "train_after2monthdata = train_after2monthdata[train_after2monthdata['post_id'].isin(train_rawdata['post_id'])]\n",
    "test_after2monthdata = test_after2monthdata[test_after2monthdata['post_id'].isin(test_rawdata['post_id'])]\n",
    "\n",
    "# Custom Dataset Class\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, rawdata, after2monthdata, image_dir, transform=None, max_images=1):\n",
    "        self.rawdata = rawdata\n",
    "        self.after2monthdata = after2monthdata\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.max_images = max_images\n",
    "        self.data = self._prepare_data()\n",
    "    \n",
    "    def _prepare_data(self):\n",
    "        data = []\n",
    "        for _, row in tqdm(self.rawdata.iterrows(), total=self.rawdata.shape[0], desc=\"Processing data\"):\n",
    "            poster_id = row['poster_id']\n",
    "            post_id = row['post_id']\n",
    "            image_files = [f for f in os.listdir(self.image_dir) if f\"{poster_id}_{post_id}\" in f]\n",
    "            images = []\n",
    "            for image_file in image_files[:self.max_images]:\n",
    "                image_path = os.path.join(self.image_dir, image_file)\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "                if self.transform:\n",
    "                    image = self.transform(image)\n",
    "                images.append(image)\n",
    "            # If less than max_images, pad with zeros\n",
    "            while len(images) < self.max_images:\n",
    "                images.append(torch.zeros((3, 224, 224)))\n",
    "            if images:\n",
    "                summary = row['summary']\n",
    "                label_data = self.after2monthdata[self.after2monthdata['post_id'] == post_id]['trend']\n",
    "                if not label_data.empty:\n",
    "                    label = label_data.values[0]\n",
    "                    # Convert label to one-hot encoding\n",
    "                    one_hot_label = F.one_hot(torch.tensor(label + 1), num_classes=3)  # -1, 0, 1 -> 0, 1, 2\n",
    "                    data.append((summary, images, one_hot_label))\n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        summary, images, label = self.data[idx]\n",
    "        return summary, torch.stack(images).float(), label.float()\n",
    "\n",
    "# Image Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create Datasets\n",
    "train_dataset = MultimodalDataset(train_rawdata, train_after2monthdata, image_dir, transform=transform)\n",
    "test_dataset = MultimodalDataset(test_rawdata, test_after2monthdata, image_dir, transform=transform)\n",
    "\n",
    "# Create Data Loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dxw/anaconda3/envs/agent/lib/python3.8/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n",
      "/home/dxw/anaconda3/envs/agent/lib/python3.8/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/home/dxw/anaconda3/envs/agent/lib/python3.8/site-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n",
      "Training Epoch 1/10: 100%|██████████| 10/10 [00:06<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.6163473397493362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/10: 100%|██████████| 10/10 [00:05<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 0.5370518237352371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/10: 100%|██████████| 10/10 [00:04<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 0.4289282463490963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/10: 100%|██████████| 10/10 [00:04<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 0.4406140327453613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/10: 100%|██████████| 10/10 [00:04<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 0.6131793782114983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/10: 100%|██████████| 10/10 [00:04<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 0.48026893734931947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/10: 100%|██████████| 10/10 [00:04<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 0.5212478071451188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/10: 100%|██████████| 10/10 [00:04<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 0.43335051089525223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9/10: 100%|██████████| 10/10 [00:04<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 0.4291202686727047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10/10: 100%|██████████| 10/10 [00:05<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 0.5224921770393849\n",
      "Model training completed and saved!\n"
     ]
    }
   ],
   "source": [
    "# Model Definition\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from imagebind import data\n",
    "from imagebind.models import imagebind_model\n",
    "from imagebind.models.imagebind_model import ModalityType\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load imagebind model\n",
    "imagebind_model = imagebind_model.imagebind_huge(pretrained=True)\n",
    "imagebind_model.eval()\n",
    "imagebind_model.to(device)\n",
    "\n",
    "class CrossAttentionFusion(nn.Module):\n",
    "    def __init__(self, text_embedding_dim, vision_embedding_dim, common_embedding_dim, num_heads):\n",
    "        super(CrossAttentionFusion, self).__init__()\n",
    "        self.text_linear = nn.Linear(text_embedding_dim, common_embedding_dim)\n",
    "        self.vision_linear = nn.Linear(vision_embedding_dim, common_embedding_dim)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=common_embedding_dim, nhead=num_heads), num_layers=2)\n",
    "        self.lstm = nn.LSTM(common_embedding_dim, common_embedding_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(common_embedding_dim, 3)  # Three classes\n",
    "\n",
    "    def forward(self, text_embeddings, vision_embeddings):\n",
    "        text_embeddings = self.text_linear(text_embeddings)\n",
    "        vision_embeddings = self.vision_linear(vision_embeddings)\n",
    "        # Flatten embeddings to match Transformer input requirements\n",
    "        text_embeddings = text_embeddings.view(text_embeddings.size(0), -1, text_embeddings.size(-1))\n",
    "        vision_embeddings = vision_embeddings.view(vision_embeddings.size(0), -1, vision_embeddings.size(-1))\n",
    "        multimodal_embeddings = torch.cat((text_embeddings, vision_embeddings), dim=1)\n",
    "        multimodal_embeddings = self.transformer_encoder(multimodal_embeddings)\n",
    "        lstm_out, _ = self.lstm(multimodal_embeddings)\n",
    "        lstm_out = lstm_out[:, -1, :]  # Take output of the last time step\n",
    "        output = self.fc(lstm_out)\n",
    "        return output\n",
    "\n",
    "def get_embeddings(text_list, image_tensors):\n",
    "    inputs = {\n",
    "        ModalityType.TEXT: data.load_and_transform_text(text_list, device),\n",
    "        ModalityType.VISION: image_tensors.to(device)\n",
    "    }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embeddings = imagebind_model(inputs)\n",
    "    \n",
    "    # Check for NaNs and Infinities in embeddings\n",
    "    if torch.isnan(embeddings[ModalityType.TEXT]).any() or torch.isinf(embeddings[ModalityType.TEXT]).any():\n",
    "        print(\"NaN or Infinity values found in text embeddings\")\n",
    "    if torch.isnan(embeddings[ModalityType.VISION]).any() or torch.isinf(embeddings[ModalityType.VISION]).any():\n",
    "        print(\"NaN or Infinity values found in vision embeddings\")\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    log_file = open(\"log.txt\", \"w\")\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n",
    "            summaries, images, labels = batch\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            text_embeddings = get_embeddings(summaries, images)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(text_embeddings[ModalityType.TEXT], text_embeddings[ModalityType.VISION])\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        log_file.write(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss}\\n')\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss}')\n",
    "    log_file.close()\n",
    "\n",
    "# Initialize model, criterion, and optimizer\n",
    "text_embedding_dim = 1024\n",
    "vision_embedding_dim = 1024\n",
    "common_embedding_dim = 768\n",
    "num_heads = 8\n",
    "\n",
    "model = CrossAttentionFusion(text_embedding_dim, vision_embedding_dim, common_embedding_dim, num_heads).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), os.path.join(data_dir, \"multimodal_model.pth\"))\n",
    "\n",
    "print(\"Model training completed and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the function to evaluate the model\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            summaries, images, labels = batch\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            text_embeddings = get_embeddings(summaries, images)\n",
    "            outputs = model(text_embeddings[ModalityType.TEXT], text_embeddings[ModalityType.VISION])\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(torch.argmax(labels, dim=1).cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    return accuracy\n",
    "\n",
    "# Load the trained model\n",
    "model_path = os.path.join(data_dir, \"multimodal_model.pth\")\n",
    "model = CrossAttentionFusion(text_embedding_dim, vision_embedding_dim, common_embedding_dim, num_heads).to(device)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_accuracy = evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
