{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: ['0', '1', '2']\n",
      "Batch 2: ['3']\n",
      "Batch 3: ['4', '5', '6']\n",
      "Batch 4: ['7', '8', '9', '10']\n",
      "Batch 5: ['11']\n",
      "Batch 6: ['12']\n",
      "Batch 7: ['13', '14']\n",
      "Batch 8: ['15']\n",
      "Batch 9: ['16']\n",
      "Batch 10: ['17']\n",
      "Batch 11: ['18', '19', '20', '21']\n",
      "Batch 12: ['22']\n",
      "Batch 13: ['23']\n",
      "Batch 14: ['24', '25']\n",
      "Batch 15: ['26']\n",
      "Batch 16: ['27', '28', '29', '30', '31', '32', '33']\n",
      "Batch 17: ['34', '35', '36', '37', '38', '39', '40', '41']\n",
      "Batch 18: ['42']\n",
      "Batch 19: ['43']\n",
      "Batch 20: ['44']\n",
      "Batch 21: ['45']\n",
      "Batch 22: ['46']\n",
      "Batch 23: ['47']\n",
      "Batch 24: ['48']\n",
      "Batch 25: ['49', '50', '51']\n",
      "Batch 26: ['52']\n",
      "Batch 27: ['53', '54']\n",
      "Batch 28: ['55']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# 加载文本向量化模型\n",
    "tokenizer = BertTokenizer.from_pretrained('/data1/dxw_data/llm/text2vec-large-chinese')\n",
    "model = BertModel.from_pretrained('/data1/dxw_data/llm/text2vec-large-chinese')\n",
    "\n",
    "def get_vector(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "# 读取 JSON 文件\n",
    "with open('./responses_caption.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 计算所有描述的向量\n",
    "vectors = []\n",
    "keys = list(data.keys())\n",
    "for key in keys:\n",
    "    text = data[key]\n",
    "    vectors.append(get_vector(text))\n",
    "\n",
    "vectors = np.vstack(vectors)\n",
    "\n",
    "# 计算相邻元素的相似度并进行分批次处理\n",
    "batches = []\n",
    "current_batch = [keys[0]]\n",
    "threshold = 0.8  # 相似度阈值，可以根据需要调整\n",
    "\n",
    "for i in range(1, len(vectors)):\n",
    "    sim = cosine_similarity(vectors[i-1].reshape(1, -1), vectors[i].reshape(1, -1))[0, 0]\n",
    "    if sim >= threshold:\n",
    "        current_batch.append(keys[i])\n",
    "    else:\n",
    "        batches.append(current_batch)\n",
    "        current_batch = [keys[i]]\n",
    "\n",
    "# 最后一批处理\n",
    "if current_batch:\n",
    "    batches.append(current_batch)\n",
    "\n",
    "# 输出批次结果\n",
    "for idx, batch in enumerate(batches):\n",
    "    print(f\"Batch {idx + 1}: {batch}\")\n",
    "\n",
    "# 保存结果到文件\n",
    "with open('/data1/dxw_data/llm/tiktok/batches.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(batches, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged JSON file saved to: /data1/dxw_data/llm/tiktok/merged_captions.json\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "import json\n",
    "\n",
    "# 定义读取和合并JSON文件的函数\n",
    "def merge_json(batches_file, captions_file, comments_file):\n",
    "    with open(batches_file) as bf:\n",
    "        batches = json.load(bf)\n",
    "\n",
    "    with open(captions_file) as cf:\n",
    "        captions = json.load(cf)\n",
    "    \n",
    "    with open(comments_file) as cmf:\n",
    "        comments = json.load(cmf)\n",
    "\n",
    "    merged_data = {}\n",
    "\n",
    "    for batch in batches:\n",
    "        combined_caption = \" \".join([captions[item] for item in batch])\n",
    "        combined_comment = \" \".join([comments[item] for item in batch])\n",
    "        merged_data[batch[0]] = {\n",
    "            \"caption\": combined_caption,\n",
    "            \"comment\": combined_comment\n",
    "        }\n",
    "\n",
    "    return merged_data\n",
    "\n",
    "# 读取和合并JSON文件\n",
    "batches_file_path = '/data1/dxw_data/llm/tiktok/batches.json'\n",
    "captions_file_path = '/data1/dxw_data/llm/tiktok/data/output_txt/responses_caption.json'\n",
    "comments_file_path = '/data1/dxw_data/llm/tiktok/data/output_txt/responses_comments.json'\n",
    "merged_data = merge_json(batches_file_path, captions_file_path, comments_file_path)\n",
    "\n",
    "# 将合并后的数据保存到新的JSON文件中\n",
    "merged_data_file_path = \"/data1/dxw_data/llm/tiktok/merged_captions.json\"\n",
    "with open(merged_data_file_path, 'w', encoding='utf-8') as merged_file:\n",
    "    json.dump(merged_data, merged_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Merged JSON file saved to: {merged_data_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
