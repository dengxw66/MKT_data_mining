{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# 读取Excel文件\n",
    "excel_path = '/data1/dxw_data/llm/taobao/true/cleaned_titles_last4.xlsx'  # 替换为你的Excel文件路径\n",
    "sheet_name = 'Sheet1'  # 替换为你的Sheet名称\n",
    "\n",
    "# 读取指定列\n",
    "df = pd.read_excel(excel_path, sheet_name=sheet_name, usecols=['name4'])\n",
    "\n",
    "# 将列数据转换为列表，并处理datetime对象\n",
    "def convert_to_serializable(val):\n",
    "    if isinstance(val, pd.Timestamp):\n",
    "        return val.strftime('%Y-%m-%d')\n",
    "    return str(val)  # 将所有非字符串类型转换为字符串\n",
    "\n",
    "title_4_list = df['name4'].dropna().apply(convert_to_serializable).tolist()\n",
    "\n",
    "# 保存为JSON文件\n",
    "json_path = '/data1/dxw_data/llm/taobao/true/title_all.json'  # 替换为你的输出JSON文件路径\n",
    "with open(json_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(title_4_list, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"数据已保存到 {json_path}！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def load_words_from_json(json_file):\n",
    "    with open(json_file, 'r', encoding='utf-8') as file:\n",
    "        words = json.load(file)\n",
    "    return words\n",
    "\n",
    "def cluster_words(words, model_name, n_clusters=500):\n",
    "    # Load the embedding model\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    # Get embeddings for the words\n",
    "    embeddings = model.encode(words, show_progress_bar=True)\n",
    "\n",
    "    # Perform clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    clusters = kmeans.fit_predict(embeddings)\n",
    "\n",
    "    return clusters\n",
    "\n",
    "def save_clusters_to_csv(words, clusters, output_csv):\n",
    "    df = pd.DataFrame({'word': words, 'cluster': clusters})\n",
    "    df.to_csv(output_csv, index=False, encoding='utf-8')\n",
    "\n",
    "# Usage example\n",
    "# json_file = '/data1/dxw_data/llm/taobao/input.json'\n",
    "# output_csv = '/data1/dxw_data/llm/taobao/output_products_category.csv'\n",
    "json_file = '/data1/dxw_data/llm/taobao/true/title_all.json'\n",
    "output_csv = '/data1/dxw_data/llm/taobao/true/all_products_category.csv'\n",
    "model_name = '/data1/dxw_data/llm/text2vec-large-chinese'\n",
    "n_clusters = 60\n",
    "\n",
    "words = load_words_from_json(json_file)\n",
    "clusters = cluster_words(words, model_name, n_clusters)\n",
    "save_clusters_to_csv(words, clusters, output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def group_words_by_cluster(csv_file, output_json, threshold=5):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Group words by their cluster\n",
    "    cluster_dict = {}\n",
    "    for _, row in df.iterrows():\n",
    "        word, cluster = row['word'], row['cluster']\n",
    "        if cluster not in cluster_dict:\n",
    "            cluster_dict[cluster] = []\n",
    "        cluster_dict[cluster].append(word)\n",
    "\n",
    "    # Filter out clusters that have fewer than the specified threshold number of words\n",
    "    filtered_cluster_dict = {k: v for k, v in cluster_dict.items() if len(v) >= threshold}\n",
    "\n",
    "    # Save the filtered grouped data to a JSON file\n",
    "    with open(output_json, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(filtered_cluster_dict, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Usage example\n",
    "csv_file = '/data1/dxw_data/llm/taobao/true/all_products_category.csv'\n",
    "output_json = '/data1/dxw_data/llm/taobao/true/all_grouped_products_category.json'\n",
    "threshold = 0\n",
    "\n",
    "group_words_by_cluster(csv_file, output_json, threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def load_clusters_from_json(json_file):\n",
    "    with open(json_file, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def cluster_subtypes(words, model_name, n_clusters=2):\n",
    "    # Load the embedding model\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    # Get embeddings for the words\n",
    "    embeddings = model.encode(words, show_progress_bar=True)\n",
    "\n",
    "    # Perform clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    clusters = kmeans.fit_predict(embeddings)\n",
    "\n",
    "    return clusters\n",
    "\n",
    "def perform_second_level_clustering(input_json, output_json, model_name):\n",
    "    # Load the existing clusters\n",
    "    existing_clusters = load_clusters_from_json(input_json)\n",
    "\n",
    "    # Initialize a dictionary to hold the second level clusters\n",
    "    second_level_clusters = {}\n",
    "\n",
    "    # Perform clustering on each cluster to divide it into sub-clusters\n",
    "    for cluster_id, words in existing_clusters.items():\n",
    "        sub_clusters = cluster_subtypes(words, model_name, n_clusters=3)\n",
    "\n",
    "        # Create sub-cluster dictionaries\n",
    "        sub_cluster_dict = {f\"{cluster_id}_{sub_cluster_id}\": [] for sub_cluster_id in range(3)}\n",
    "        for word, sub_cluster_id in zip(words, sub_clusters):\n",
    "            sub_cluster_dict[f\"{cluster_id}_{sub_cluster_id}\"].append(word)\n",
    "        \n",
    "        # Update the main dictionary with sub-clusters\n",
    "        second_level_clusters.update(sub_cluster_dict)\n",
    "\n",
    "    # Save the resulting sub-clusters into a JSON file\n",
    "    with open(output_json, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(second_level_clusters, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Usage example\n",
    "input_json = '/data1/dxw_data/llm/taobao/true/all_grouped_products_category.json'\n",
    "output_json = '/data1/dxw_data/llm/taobao/true/all_output_sub_clusters3.json'\n",
    "model_name = '/data1/dxw_data/llm/text2vec-large-chinese'\n",
    "\n",
    "perform_second_level_clustering(input_json, output_json, model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Path to the input JSON file\n",
    "input_file_path = \"/data1/dxw_data/llm/taobao/true/all_output_sub_clusters3.json\"\n",
    "output_file_path = \"/data1/dxw_data/llm/taobao/true/all_output_sub_clusters3-litte2.json\"\n",
    "\n",
    "# Read the input JSON data from the file\n",
    "with open(input_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    input_data = json.load(file)\n",
    "\n",
    "# Initialize the output dictionary\n",
    "output_data = {}\n",
    "\n",
    "# Iterate over each type in the input data\n",
    "for key, values in input_data.items():\n",
    "    if isinstance(values, list):\n",
    "        # Randomly sample 20 elements for each type\n",
    "        output_data[key] = random.sample(values, min(20, len(values)))\n",
    "    else:\n",
    "        print(f\"Skipping key {key} because its value is not a list.\")\n",
    "\n",
    "# Convert the output dictionary to JSON with UTF-8 encoding\n",
    "output_json = json.dumps(output_data, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Save the output JSON to a file\n",
    "with open(output_file_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json_file.write(output_json)\n",
    "\n",
    "# Print the output JSON\n",
    "print(output_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt 生成分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load the JSON files\n",
    "with open('/data1/dxw_data/llm/taobao/true/all_output_sub_clusters3.json', 'r', encoding='utf-8') as file:\n",
    "    sub_clusters = json.load(file)\n",
    "\n",
    "with open('/data1/dxw_data/llm/taobao/true/updated_categories.json', 'r', encoding='utf-8') as file:\n",
    "    official_categories = json.load(file)\n",
    "\n",
    "# Create a list to hold the output data\n",
    "output_data = []\n",
    "\n",
    "# Process each item in sub_clusters and assign categories\n",
    "for cluster_key, items in sub_clusters.items():\n",
    "    if cluster_key in official_categories:\n",
    "        major_category = official_categories[cluster_key][0]\n",
    "        minor_category = official_categories[cluster_key][1]\n",
    "    else:\n",
    "        major_category = \"Unknown\"\n",
    "        minor_category = \"Unknown\"\n",
    "    \n",
    "    for item in items:\n",
    "        output_data.append({\n",
    "            \"item\": item,\n",
    "            \"major_category\": major_category,\n",
    "            \"minor_category\": minor_category\n",
    "        })\n",
    "\n",
    "# Convert the output data to a DataFrame\n",
    "df = pd.DataFrame(output_data)\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "output_path = '/data1/dxw_data/llm/taobao/true/assigned_categories.xlsx'\n",
    "df.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\"Categories assigned and saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_excel columns: Index(['name', 'cleaned_title4'], dtype='object')\n",
      "b_excel columns: Index(['name', 'major_category', 'minor_category'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging DataFrames: 100%|██████████| 1/1 [00:00<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched_data columns: Index(['name_x', 'cleaned_title4', 'name_y', 'major_category',\n",
      "       'minor_category'],\n",
      "      dtype='object')\n",
      "'name'列不存在，跳过删除步骤\n",
      "合并后的表格已保存到 /data1/dxw_data/llm/taobao/true/merged_output.xlsx\n"
     ]
    }
   ],
   "source": [
    "# 这个是合并的成功代码，两个excel的匹配\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 读取Excel文件\n",
    "a_excel = pd.read_excel('/data1/dxw_data/llm/taobao/true/cleaned_titles_last4.xlsx')\n",
    "b_excel = pd.read_excel('/data1/dxw_data/llm/taobao/true/assigned_categories.xlsx')\n",
    "\n",
    "# 打印列名以确认\n",
    "print(\"a_excel columns:\", a_excel.columns)\n",
    "print(\"b_excel columns:\", b_excel.columns)\n",
    "\n",
    "# 去重：保留b_excel中name列的第一个出现值\n",
    "b_excel_unique = b_excel.drop_duplicates(subset='name', keep='first')\n",
    "\n",
    "# 使用tqdm进度条进行数据合并\n",
    "with tqdm(total=1, desc=\"Merging DataFrames\") as pbar:\n",
    "    matched_data = a_excel.merge(b_excel_unique[['name', 'major_category', 'minor_category']], left_on='cleaned_title4', right_on='name', how='left')\n",
    "    pbar.update(1)\n",
    "\n",
    "# 打印合并后数据框的列名以确认\n",
    "print(\"matched_data columns:\", matched_data.columns)\n",
    "\n",
    "# 检查'name'列是否存在并删除不需要的列\n",
    "if 'name' in matched_data.columns:\n",
    "    with tqdm(total=1, desc=\"Dropping unnecessary columns\") as pbar:\n",
    "        matched_data.drop(columns=['name'], inplace=True)\n",
    "        pbar.update(1)\n",
    "else:\n",
    "    print(\"'name'列不存在，跳过删除步骤\")\n",
    "\n",
    "# 输出合并后的表格\n",
    "output_path = '/data1/dxw_data/llm/taobao/true/merged_output.xlsx'\n",
    "matched_data.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\"合并后的表格已保存到 {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 读取Excel文件\n",
    "df = pd.read_excel('/data1/dxw_data/llm/taobao/refine/huitun_room_goods_sample_xiangwen.xlsx')\n",
    "\n",
    "# 确保 tqdm 在 groupby 后显示进度条\n",
    "categories = df['小类别'].unique()\n",
    "\n",
    "sorted_groups = []\n",
    "for category in tqdm(categories, desc=\"Processing categories\"):\n",
    "    group = df[df['小类别'] == category].sort_values(by='livePrice')\n",
    "    sorted_groups.append(group)\n",
    "\n",
    "# 将所有分组结果合并\n",
    "df_sorted = pd.concat(sorted_groups).reset_index(drop=True)\n",
    "\n",
    "# 输出结果到新的Excel文件\n",
    "df_sorted.to_excel('/data1/dxw_data/llm/taobao/refine/huitun_room_goods_sample_xiangwen_sorted_output_file.xlsx', index=False)\n",
    "\n",
    "print(\"Processing complete. The sorted data has been saved to 'huitun_room_goods_sample_xiangwen_sorted_output_file.xlsx'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
