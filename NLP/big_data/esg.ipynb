{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入：tw_after_incident_0_10.xlsx和cl_news_sp500_20240105\n",
    "# 输出：tw_after_incident_0_10_origin_highest_with_matches_update-all3.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取Excel文件\n",
    "file_path = '/data1/dxw_data/llm/RA/hku_ivy/esg/cl_news_sp500_20240105.csv'  # 请替换为实际文件路径\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 读取停词表\n",
    "stopwords_file_path = '/data1/dxw_data/llm/ML/LIWC/datasets/stopwords_en.txt'\n",
    "with open(stopwords_file_path, 'r') as file:\n",
    "    stop_words = set(file.read().splitlines())\n",
    "\n",
    "# 数据清洗函数\n",
    "def clean_text(text):\n",
    "    words = text.split()  # 简单分词\n",
    "    cleaned_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(cleaned_words)\n",
    "\n",
    "# 对\"HD\"列进行数据清洗\n",
    "df['HD_cleaned'] = df['HD'].astype(str).apply(clean_text)\n",
    "\n",
    "# 保存清洗后的数据到新Excel文件\n",
    "output_file_path = '/data1/dxw_data/llm/RA/hku_ivy/esg/cleaned_data.xlsx'\n",
    "df.to_excel(output_file_path, index=False)\n",
    "\n",
    "print(f\"Cleaned data saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from summa import summarizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 读取Excel文件\n",
    "file_path = '/data1/dxw_data/llm/RA/hku_ivy/esg/cleaned_data.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# 定义过滤和摘要函数\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # 删除[]包裹的内容\n",
    "        clean_text = re.sub(r'\\[.*?\\]', '', text)\n",
    "        return clean_text\n",
    "    return ''\n",
    "\n",
    "def summarize_text(text):\n",
    "    if isinstance(text, str) and text.strip() != '':\n",
    "        # 使用summa.summarizer进行文本摘要\n",
    "        summary = summarizer.summarize(text)\n",
    "        return summary\n",
    "    return ''\n",
    "\n",
    "# 创建新列并初始化进度条\n",
    "df['Cleaned_TXT'] = ''\n",
    "df['Summary'] = ''\n",
    "total = len(df)\n",
    "progress_bar = tqdm(total=total, desc=\"Processing\", unit=\"row\")\n",
    "\n",
    "# 对TXT列进行处理\n",
    "for index, row in df.iterrows():\n",
    "    try:\n",
    "        cleaned_text = clean_text(row['TXT'])\n",
    "        summary = summarize_text(cleaned_text)\n",
    "        df.at[index, 'Cleaned_TXT'] = cleaned_text\n",
    "        df.at[index, 'Summary'] = summary\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {index}: {e}\")\n",
    "    progress_bar.update(1)\n",
    "\n",
    "# 关闭进度条\n",
    "progress_bar.close()\n",
    "\n",
    "# 保存到新的Excel文件中\n",
    "new_file_path = '/data1/dxw_data/llm/RA/hku_ivy/esg/cleaned_data_update.xlsx'\n",
    "df.to_excel(new_file_path, index=False)\n",
    "\n",
    "print(\"文本处理和摘要已完成，并保存到cleaned_data_update.xlsx中。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 读取Excel文件\n",
    "file_path = '/data1/dxw_data/llm/RA/hku_ivy/esg/cleaned_data_update.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# 初始化进度条\n",
    "total = len(df)\n",
    "progress_bar = tqdm(total=total, desc=\"Processing\", unit=\"row\")\n",
    "\n",
    "# 对Summary列进行处理，将HD_cleaned列内容插入到Summary列的最前面\n",
    "for index, row in df.iterrows():\n",
    "    try:\n",
    "        hd_cleaned_text = row['HD_cleaned']\n",
    "        summary_text = row['Summary']\n",
    "        \n",
    "        if isinstance(hd_cleaned_text, str) and isinstance(summary_text, str):\n",
    "            new_summary = f\"{hd_cleaned_text}\\n{summary_text}\"\n",
    "            df.at[index, 'Summary'] = new_summary\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {index}: {e}\")\n",
    "    progress_bar.update(1)\n",
    "\n",
    "# 关闭进度条\n",
    "progress_bar.close()\n",
    "\n",
    "# 保存到新的Excel文件中\n",
    "new_file_path = '/data1/dxw_data/llm/RA/hku_ivy/esg/cleaned_data_update_v2.xlsx'\n",
    "df.to_excel(new_file_path, index=False)\n",
    "\n",
    "print(\"Summary列更新已完成，并保存到cleaned_data_update_v2.xlsx中。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = '/data1/dxw_data/llm/RA/hku_ivy/esg/tw_after_incident_0_10.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Function to select the row with the second-highest similarity_score in each group\n",
    "def select_second_highest(group):\n",
    "    if len(group) > 1:\n",
    "        sorted_group = group.sort_values(by='similarity_score', ascending=False)\n",
    "        return sorted_group.iloc[0]  # first highest\n",
    "    else:\n",
    "        return group.iloc[0]  # If only one row, return it\n",
    "\n",
    "# Process each (company_ticker, incident_date) group independently\n",
    "result = []\n",
    "grouped = df.groupby(['company_ticker', 'incident_date'])\n",
    "\n",
    "for _, group in grouped:\n",
    "    selected_row = select_second_highest(group)\n",
    "    result.append(selected_row)\n",
    "\n",
    "# Concatenate all the selected rows\n",
    "result_df = pd.DataFrame(result)\n",
    "\n",
    "# Save the resulting DataFrame to a new file\n",
    "output_file_path = '/data1/dxw_data/llm/RA/hku_ivy/esg/tw_after_incident_0_10_origin_highest.xlsx'\n",
    "result_df.to_excel(output_file_path, index=False)\n",
    "\n",
    "print(\"Selection completed and saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个是正确代码，全过程运行最后一版，直接运行就好。目前最新进度，只有一个问题，就是有些无法匹配的就留空了。\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "import torch\n",
    "\n",
    "# 读取Excel文件\n",
    "file1_path = '/data1/dxw_data/llm/RA/hku_ivy/esg/tw_after_incident_0_10_origin_highest.xlsx'\n",
    "file2_path = '/data1/dxw_data/llm/RA/hku_ivy/esg/cleaned_data_update_v2.xlsx'\n",
    "\n",
    "df1 = pd.read_excel(file1_path)\n",
    "df2 = pd.read_excel(file2_path)\n",
    "\n",
    "# 转换日期格式\n",
    "df1['incident_date'] = pd.to_datetime(df1['incident_date'], format='%Y/%m/%d')\n",
    "df2['PD'] = pd.to_datetime(df2['PD'], format='%Y-%m-%d')\n",
    "\n",
    "# 初始化模型\n",
    "model = SentenceTransformer('/data1/dxw_data/llm/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# 创建新的列来保存匹配结果\n",
    "for i in range(1, 6):\n",
    "    df1[f'matched_summary_{i}'] = \"\"\n",
    "    df1[f'PD_match_{i}'] = pd.NaT\n",
    "    df1[f'similarity_{i}'] = 0.0\n",
    "\n",
    "def process_row(idx1, row1, df2, model):\n",
    "    try:\n",
    "        incident_date_plus_15 = row1['incident_date'] + timedelta(days=15)\n",
    "        matching_rows = df2[(df2['ticker'] == row1['company_ticker']) & \n",
    "                            (df2['PD'] >= row1['incident_date']) & \n",
    "                            (df2['PD'] <= incident_date_plus_15)]\n",
    "        if matching_rows.empty:\n",
    "            return idx1, None\n",
    "        \n",
    "        incident_words = row1['incident_category']\n",
    "        incident_embeddings = [model.encode(word, convert_to_tensor=True) for word in incident_words]\n",
    "        \n",
    "        summary_sentences = matching_rows['Summary'].dropna().astype(str).str.split('\\n').explode()\n",
    "        summary_sentences = summary_sentences[summary_sentences.apply(lambda x: isinstance(x, str))]\n",
    "        \n",
    "        if summary_sentences.empty:\n",
    "            return idx1, None\n",
    "        \n",
    "        summary_embeddings = model.encode(summary_sentences.tolist(), convert_to_tensor=True)\n",
    "        \n",
    "        total_similarities = []\n",
    "        for summary_embedding in summary_embeddings:\n",
    "            max_similarity = max(util.pytorch_cos_sim(incident_embedding, summary_embedding).item() for incident_embedding in incident_embeddings)\n",
    "            total_similarities.append(max_similarity)\n",
    "        \n",
    "        summary_sentence_similarities = pd.Series(total_similarities, index=summary_sentences.index)\n",
    "        \n",
    "        matching_rows = matching_rows.join(summary_sentence_similarities.groupby(summary_sentence_similarities.index).sum().rename('total_similarity'))\n",
    "        \n",
    "        top_5_matching_rows = matching_rows.nlargest(5, 'total_similarity')\n",
    "        \n",
    "        results = []\n",
    "        for i, row in enumerate(top_5_matching_rows.itertuples(), start=1):\n",
    "            results.append((idx1, i, row.Summary, row.PD, row.total_similarity))\n",
    "        \n",
    "        return idx1, results\n",
    "    except RuntimeError as e:\n",
    "        if 'CUDA out of memory' in str(e):\n",
    "            print(f\"CUDA out of memory error at row {idx1}. Trying to free memory.\")\n",
    "            torch.cuda.empty_cache()\n",
    "            return idx1, None\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "# 提取需要计算相似度的行\n",
    "valid_rows = [(idx1, row1, df2, model) for idx1, row1 in df1.iterrows()]\n",
    "\n",
    "# 并行处理\n",
    "results = Parallel(n_jobs=4)(delayed(process_row)(idx1, row1, df2, model) for idx1, row1, df2, model in tqdm(valid_rows, desc=\"Calculating similarities\"))\n",
    "\n",
    "# 处理结果\n",
    "for idx1, res in results:\n",
    "    if res is not None:\n",
    "        for idx1, i, summary, pd_match, similarity in res:\n",
    "            df1.at[idx1, f'matched_summary_{i}'] = summary\n",
    "            df1.at[idx1, f'PD_match_{i}'] = pd_match\n",
    "            df1.at[idx1, f'similarity_{i}'] = similarity\n",
    "\n",
    "# 保存结果到新的Excel文件\n",
    "output_file_path = '/data1/dxw_data/llm/RA/hku_ivy/esg/tw_after_incident_0_10_origin_highest_with_matches_update-all3.xlsx'\n",
    "df1.to_excel(output_file_path, index=False)\n",
    "\n",
    "print(f\"结果已保存到 {output_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
