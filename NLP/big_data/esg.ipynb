{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入：tw_after_incident_0_10.xlsx和cl_news_sp500_20240105\n",
    "# 输出：tw_after_incident_0_10_origin_highest_with_matches_update-all3.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取Excel文件\n",
    "file_path = '/data1/dxw_data/llm/RA/hku_ivy/esg/cl_news_sp500_20240105.csv'  # 请替换为实际文件路径\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 读取停词表\n",
    "stopwords_file_path = '/data1/dxw_data/llm/ML/LIWC/datasets/stopwords_en.txt'\n",
    "with open(stopwords_file_path, 'r') as file:\n",
    "    stop_words = set(file.read().splitlines())\n",
    "\n",
    "# 数据清洗函数\n",
    "def clean_text(text):\n",
    "    words = text.split()  # 简单分词\n",
    "    cleaned_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(cleaned_words)\n",
    "\n",
    "# 对\"HD\"列进行数据清洗\n",
    "df['HD_cleaned'] = df['HD'].astype(str).apply(clean_text)\n",
    "\n",
    "# 保存清洗后的数据到新Excel文件\n",
    "output_file_path = '/data1/dxw_data/llm/RA/hku_ivy/esg/cleaned_data.xlsx'\n",
    "df.to_excel(output_file_path, index=False)\n",
    "\n",
    "print(f\"Cleaned data saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from summa import summarizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 读取Excel文件\n",
    "file_path = '/data1/dxw_data/llm/RA/hku_ivy/esg/cleaned_data.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# 定义过滤和摘要函数\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # 删除[]包裹的内容\n",
    "        clean_text = re.sub(r'\\[.*?\\]', '', text)\n",
    "        return clean_text\n",
    "    return ''\n",
    "\n",
    "def summarize_text(text):\n",
    "    if isinstance(text, str) and text.strip() != '':\n",
    "        # 使用summa.summarizer进行文本摘要\n",
    "        summary = summarizer.summarize(text)\n",
    "        return summary\n",
    "    return ''\n",
    "\n",
    "# 创建新列并初始化进度条\n",
    "df['Cleaned_TXT'] = ''\n",
    "df['Summary'] = ''\n",
    "total = len(df)\n",
    "progress_bar = tqdm(total=total, desc=\"Processing\", unit=\"row\")\n",
    "\n",
    "# 对TXT列进行处理\n",
    "for index, row in df.iterrows():\n",
    "    try:\n",
    "        cleaned_text = clean_text(row['TXT'])\n",
    "        summary = summarize_text(cleaned_text)\n",
    "        df.at[index, 'Cleaned_TXT'] = cleaned_text\n",
    "        df.at[index, 'Summary'] = summary\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {index}: {e}\")\n",
    "    progress_bar.update(1)\n",
    "\n",
    "# 关闭进度条\n",
    "progress_bar.close()\n",
    "\n",
    "# 保存到新的Excel文件中\n",
    "new_file_path = '/data1/dxw_data/llm/RA/hku_ivy/esg/cleaned_data_update.xlsx'\n",
    "df.to_excel(new_file_path, index=False)\n",
    "\n",
    "print(\"文本处理和摘要已完成，并保存到cleaned_data_update.xlsx中。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 读取Excel文件\n",
    "file_path = '/data1/dxw_data/llm/RA/hku_ivy/esg/cleaned_data_update.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# 初始化进度条\n",
    "total = len(df)\n",
    "progress_bar = tqdm(total=total, desc=\"Processing\", unit=\"row\")\n",
    "\n",
    "# 对Summary列进行处理，将HD_cleaned列内容插入到Summary列的最前面\n",
    "for index, row in df.iterrows():\n",
    "    try:\n",
    "        hd_cleaned_text = row['HD_cleaned']\n",
    "        summary_text = row['Summary']\n",
    "        \n",
    "        if isinstance(hd_cleaned_text, str) and isinstance(summary_text, str):\n",
    "            new_summary = f\"{hd_cleaned_text}\\n{summary_text}\"\n",
    "            df.at[index, 'Summary'] = new_summary\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {index}: {e}\")\n",
    "    progress_bar.update(1)\n",
    "\n",
    "# 关闭进度条\n",
    "progress_bar.close()\n",
    "\n",
    "# 保存到新的Excel文件中\n",
    "new_file_path = '/data1/dxw_data/llm/RA/hku_ivy/esg/cleaned_data_update_v2.xlsx'\n",
    "df.to_excel(new_file_path, index=False)\n",
    "\n",
    "print(\"Summary列更新已完成，并保存到cleaned_data_update_v2.xlsx中。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'similarity_score'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_459041/1513369911.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;31m# Load the Excel file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/data1/dxw_data/llm/RA/hku_ivy/esg/tw_after_incident_0_10.xlsx'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_459041/1513369911.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(group)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mselect_second_highest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0msorted_group\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'similarity_score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msorted_group\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# first highest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# If only one row, return it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/agent/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[1;32m   6754\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6755\u001b[0m             \u001b[0;31m# len(by) == 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6756\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6757\u001b[0m             \u001b[0mby\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6758\u001b[0;31m             \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6760\u001b[0m             \u001b[0;31m# need to rewrap column in Series to apply key function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6761\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/agent/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1774\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1776\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1778\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1780\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1781\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'similarity_score'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = '/data1/dxw_data/llm/RA/hku_ivy/esg/tw_after_incident_0_10.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Function to select the row with the second-highest similarity_score in each group\n",
    "def select_second_highest(group):\n",
    "    if len(group) > 1:\n",
    "        sorted_group = group.sort_values(by='similarity_score', ascending=False)\n",
    "        return sorted_group.iloc[0]  # first highest\n",
    "    else:\n",
    "        return group.iloc[0]  # If only one row, return it\n",
    "\n",
    "# Process each (company_ticker, incident_date) group independently\n",
    "result = []\n",
    "grouped = df.groupby(['company_ticker', 'incident_date'])\n",
    "\n",
    "for _, group in grouped:\n",
    "    selected_row = select_second_highest(group)\n",
    "    result.append(selected_row)\n",
    "\n",
    "# Concatenate all the selected rows\n",
    "result_df = pd.DataFrame(result)\n",
    "\n",
    "# Save the resulting DataFrame to a new file\n",
    "output_file_path = '/data1/dxw_data/llm/RA/hku_ivy/esg/tw_after_incident_0_10_origin_highest.xlsx'\n",
    "result_df.to_excel(output_file_path, index=False)\n",
    "\n",
    "print(\"Selection completed and saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-11 12:29:42.129025: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-11 12:29:42.279972: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-11 12:29:42.929888: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64\n",
      "2024-07-11 12:29:42.929978: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64\n",
      "2024-07-11 12:29:42.929984: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Calculating similarities:   2%|▏         | 4/200 [00:00<00:17, 11.16it/s]2024-07-11 12:29:54.669746: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-11 12:29:54.828000: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-11 12:29:55.478845: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64\n",
      "2024-07-11 12:29:55.478926: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64\n",
      "2024-07-11 12:29:55.478933: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2024-07-11 12:29:57.938476: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-11 12:29:58.077245: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-11 12:29:58.756487: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64\n",
      "2024-07-11 12:29:58.756565: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64\n",
      "2024-07-11 12:29:58.756572: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2024-07-11 12:30:02.025667: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Calculating similarities:   4%|▍         | 8/200 [00:17<08:23,  2.62s/it]2024-07-11 12:30:02.172105: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-11 12:30:02.896161: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64\n",
      "2024-07-11 12:30:02.896245: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64\n",
      "2024-07-11 12:30:02.896252: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2024-07-11 12:30:05.881238: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-11 12:30:06.052591: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-11 12:30:06.733905: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64\n",
      "2024-07-11 12:30:06.733978: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64\n",
      "2024-07-11 12:30:06.733984: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Calculating similarities:  12%|█▏        | 24/200 [01:21<12:25,  4.24s/it]/home/dxw/anaconda3/envs/agent/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "Calculating similarities:  14%|█▍        | 29/200 [01:38<10:43,  3.76s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 80\u001b[0m\n\u001b[1;32m     77\u001b[0m valid_rows \u001b[38;5;241m=\u001b[39m [(idx1, row1, df2, model) \u001b[38;5;28;01mfor\u001b[39;00m idx1, row1 \u001b[38;5;129;01min\u001b[39;00m df1\u001b[38;5;241m.\u001b[39miterrows()]\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# 并行处理\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_row\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalid_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCalculating similarities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# 处理结果\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx1, res \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "File \u001b[0;32m~/anaconda3/envs/agent/lib/python3.8/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/agent/lib/python3.8/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/agent/lib/python3.8/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 这个是正确代码，全过程运行最后一版，直接运行就好。目前最新进度，只有一个问题，就是有些无法匹配的就留空了。\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "import torch\n",
    "\n",
    "# 读取Excel文件\n",
    "file1_path = '/data1/dxw_data/llm/RA/hku_ivy/esg/tw_after_incident_0_10_origin_highest.xlsx'\n",
    "# file1_path = '/data1/dxw_data/llm/RA/hku_ivy/esg/tw_after_incident_0_10.xlsx'\n",
    "file2_path = '/data1/dxw_data/llm/RA/hku_ivy/esg/cleaned_data_update_v2.xlsx'\n",
    "\n",
    "df1 = pd.read_excel(file1_path, nrows=200)\n",
    "df2 = pd.read_excel(file2_path)\n",
    "\n",
    "# 转换日期格式\n",
    "df1['incident_date'] = pd.to_datetime(df1['incident_date'], format='%Y/%m/%d')\n",
    "df2['PD'] = pd.to_datetime(df2['PD'], format='%Y-%m-%d')\n",
    "\n",
    "# 初始化模型\n",
    "model = SentenceTransformer('/data1/dxw_data/llm/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# 创建新的列来保存匹配结果\n",
    "for i in range(1, 6):\n",
    "    df1[f'matched_summary_{i}'] = \"\"\n",
    "    df1[f'PD_match_{i}'] = pd.NaT\n",
    "    df1[f'similarity_{i}'] = 0.0\n",
    "\n",
    "def process_row(idx1, row1, df2, model):\n",
    "    try:\n",
    "        incident_date_plus_15 = row1['incident_date'] + timedelta(days=15)\n",
    "        matching_rows = df2[(df2['ticker'] == row1['company_ticker']) & \n",
    "                            (df2['PD'] >= row1['incident_date']) & \n",
    "                            (df2['PD'] <= incident_date_plus_15)]\n",
    "        if matching_rows.empty:\n",
    "            return idx1, None\n",
    "        \n",
    "        incident_words = row1['incident_category']\n",
    "        incident_embeddings = [model.encode(word, convert_to_tensor=True) for word in incident_words]\n",
    "        \n",
    "        summary_sentences = matching_rows['Summary'].dropna().astype(str).str.split('\\n').explode()\n",
    "        summary_sentences = summary_sentences[summary_sentences.apply(lambda x: isinstance(x, str))]\n",
    "        \n",
    "        if summary_sentences.empty:\n",
    "            return idx1, None\n",
    "        \n",
    "        summary_embeddings = model.encode(summary_sentences.tolist(), convert_to_tensor=True)\n",
    "        \n",
    "        total_similarities = []\n",
    "        for summary_embedding in summary_embeddings:\n",
    "            max_similarity = max(util.pytorch_cos_sim(incident_embedding, summary_embedding).item() for incident_embedding in incident_embeddings)\n",
    "            total_similarities.append(max_similarity)\n",
    "        \n",
    "        summary_sentence_similarities = pd.Series(total_similarities, index=summary_sentences.index)\n",
    "        \n",
    "        matching_rows = matching_rows.join(summary_sentence_similarities.groupby(summary_sentence_similarities.index).sum().rename('total_similarity'))\n",
    "        \n",
    "        top_5_matching_rows = matching_rows.nlargest(5, 'total_similarity')\n",
    "        \n",
    "        results = []\n",
    "        for i, row in enumerate(top_5_matching_rows.itertuples(), start=1):\n",
    "            results.append((idx1, i, row.Summary, row.PD, row.total_similarity))\n",
    "        \n",
    "        return idx1, results\n",
    "    except RuntimeError as e:\n",
    "        if 'CUDA out of memory' in str(e):\n",
    "            print(f\"CUDA out of memory error at row {idx1}. Trying to free memory.\")\n",
    "            torch.cuda.empty_cache()\n",
    "            return idx1, None\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "# 提取需要计算相似度的行\n",
    "valid_rows = [(idx1, row1, df2, model) for idx1, row1 in df1.iterrows()]\n",
    "\n",
    "# 并行处理\n",
    "results = Parallel(n_jobs=4)(delayed(process_row)(idx1, row1, df2, model) for idx1, row1, df2, model in tqdm(valid_rows, desc=\"Calculating similarities\"))\n",
    "\n",
    "# 处理结果\n",
    "for idx1, res in results:\n",
    "    if res is not None:\n",
    "        for idx1, i, summary, pd_match, similarity in res:\n",
    "            df1.at[idx1, f'matched_summary_{i}'] = summary\n",
    "            df1.at[idx1, f'PD_match_{i}'] = pd_match\n",
    "            df1.at[idx1, f'similarity_{i}'] = similarity\n",
    "\n",
    "# 保存结果到新的Excel文件\n",
    "output_file_path = '/data1/dxw_data/llm/RA/hku_ivy/esg/tw_after_incident_0_10_origin_highest_with_matches_update-all3_test.xlsx'\n",
    "df1.to_excel(output_file_path, index=False)\n",
    "\n",
    "print(f\"结果已保存到 {output_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
