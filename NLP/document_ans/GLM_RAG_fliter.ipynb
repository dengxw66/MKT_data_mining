{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dxw22/anaconda3/envs/agentllm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# import\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain import LLMChain\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms.base import LLM\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from typing import Any, Dict, List, Mapping, Optional, Tuple, Union\n",
    "from torch.mps import empty_cache\n",
    "import torch\n",
    "from langchain.chains import RetrievalQA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLM(LLM):\n",
    "    max_token: int = 2048\n",
    "    temperature: float = 0.8\n",
    "    top_p = 0.9\n",
    "    tokenizer: object = None\n",
    "    model: object = None\n",
    "    history_len: int = 1024\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"GLM\"\n",
    "            \n",
    "    def load_model(self, llm_device=\"gpu\",model_name_or_path=None):\n",
    "        model_config = AutoConfig.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,trust_remote_code=True)\n",
    "        self.model = AutoModel.from_pretrained(model_name_or_path, config=model_config, trust_remote_code=True, device='cuda:5').half() # GLM模块装在gpu: 6\n",
    "\n",
    "\n",
    "\n",
    "    def _call(self,prompt:str,history:List[str] = [],stop: Optional[List[str]] = None):\n",
    "        response, _ = self.model.chat(\n",
    "                    self.tokenizer,prompt,\n",
    "                    history=history[-self.history_len:] if self.history_len > 0 else [],\n",
    "                    max_length=self.max_token,temperature=self.temperature,\n",
    "                    top_p=self.top_p)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 访谈记录的原始文本加载\n",
    "loader = TextLoader(\"/data1/dxw_data/code/MKT/GLM-mkt/GLM_RAG_MKT/data/Interview.txt\", encoding=\"utf-8\")\n",
    "documents = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 285, which is longer than the specified 100\n",
      "Created a chunk of size 125, which is longer than the specified 100\n",
      "Created a chunk of size 367, which is longer than the specified 100\n",
      "Created a chunk of size 168, which is longer than the specified 100\n",
      "Created a chunk of size 229, which is longer than the specified 100\n",
      "Created a chunk of size 247, which is longer than the specified 100\n",
      "Created a chunk of size 220, which is longer than the specified 100\n",
      "Created a chunk of size 133, which is longer than the specified 100\n",
      "Created a chunk of size 157, which is longer than the specified 100\n",
      "Created a chunk of size 101, which is longer than the specified 100\n",
      "Created a chunk of size 208, which is longer than the specified 100\n",
      "Created a chunk of size 120, which is longer than the specified 100\n",
      "Created a chunk of size 191, which is longer than the specified 100\n",
      "Created a chunk of size 165, which is longer than the specified 100\n",
      "Created a chunk of size 102, which is longer than the specified 100\n",
      "Created a chunk of size 277, which is longer than the specified 100\n",
      "Created a chunk of size 243, which is longer than the specified 100\n",
      "Created a chunk of size 152, which is longer than the specified 100\n",
      "Created a chunk of size 102, which is longer than the specified 100\n",
      "Created a chunk of size 145, which is longer than the specified 100\n",
      "Created a chunk of size 182, which is longer than the specified 100\n",
      "Created a chunk of size 139, which is longer than the specified 100\n",
      "Created a chunk of size 140, which is longer than the specified 100\n",
      "Created a chunk of size 121, which is longer than the specified 100\n",
      "Created a chunk of size 192, which is longer than the specified 100\n",
      "Created a chunk of size 146, which is longer than the specified 100\n",
      "No sentence-transformers model found with name /data1/dxw_data/code/MKT/GLM-mkt/text2vec-large-chinese. Creating a new one with MEAN pooling.\n"
     ]
    }
   ],
   "source": [
    "# split it into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# create the open-source embedding function\n",
    "model_kwargs = {'device': 'cuda:6'}  # embedding模块装在gpu: 7\n",
    "embedding_function = HuggingFaceEmbeddings(model_name='/data1/dxw_data/code/MKT/GLM-mkt/text2vec-large-chinese',model_kwargs=model_kwargs) # 会报错“No sentence-transformers model found”但是不影响使用,这只是huggingface的检测问题。\n",
    "\n",
    "# load it into Chroma\n",
    "db = Chroma.from_documents(docs, embedding_function)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:11<00:00,  1.71s/it]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "modelpath = \"/data1/dxw_data/code/Agent/code/ChatGLM3/chatglm3-6b\"\n",
    "sys.path.append(modelpath)\n",
    "llm = GLM()\n",
    "llm.load_model(model_name_or_path = modelpath)\n",
    "#---------------------------至此, 成功加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------开始总结数据------------------------ #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'文本内容是关于公司提供高端解决方案和设备控制技术的访谈记录。公司强调自己的技术优势在于打标速度快，基于不同的控制办法。镜片定制主要是因为影响温漂和反射率等因素。材料外采有一定要求，主要是对镀膜工艺有特别要求。大功率切割的硬件套与激光振镜相比，控制部分技术难度相近，但打标软件技术积累时间较长，靠解决工艺问题积累。'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 直接概括访谈纪要文本的内容，但效果不好，会有遗漏\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=db.as_retriever())\n",
    "query = \"根据文本的提问,概括出文本内容？\" # 根据业务逻辑\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文本内容是关于公司提供高端解决方案和设备控制技术的访谈记录。公司强调自己的技术优势在于打标速度快，基于不同的控制办法。镜片定制主要是因为影响温漂和反射率等因素。材料外采有一定要求，主要是对镀膜工艺有特别要求。大功率切割的硬件套与激光振镜相比，控制部分技术难度相近，但打标软件技术积累时间较长，靠解决工艺问题积累。\n"
     ]
    }
   ],
   "source": [
    "# print(\"文本内容是关于公司提供高端解决方案和设备控制技术的访谈记录。公司强调自己的技术优势在于打标速度快，基于不同的控制办法。镜片定制主要是因为影响温漂和反射率等因素。材料外采有一定要求，主要是对镀膜工艺有特别要求。大功率切割的硬件套与激光振镜相比，控制部分技术难度相近，但打标软件技术积累时间较长，靠解决工艺问题积累。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------尝试多步sequence-chain总结------------------------ #\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dxw22/anaconda3/envs/agentllm/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "提炼的主题:  文本的主题包括：振镜水平的高低判断，镜片定制，材料来源，大功率切割硬件套与激光振镜的比较，打标软件的技术难度，以及企业技术优势和目的。\n",
      "总结的结果:  文本主要讨论了振镜水平判断、镜片定制、材料来源、大功率切割硬件套与激光振镜的比较、打标软件的技术难度以及企业技术优势和目的。其中，振镜水平判断的方法主要是价格。镜片定制主要是因为影响温漂等因素。材料来源方面，有些是外采的，有些则需自行制作。大功率切割硬件套与激光振镜的比较主要在于控制部分的技术难度。打标软件的技术难度在于解决工艺问题积累。企业技术优势和目的是在精密加工高端领域与振镜厂商合作定制控制卡等。\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------- #\n",
    " # 业务逻辑: \n",
    "    # 1) 每次提取访谈记录的文本\n",
    "    # 2) 然后根据问题先提炼主题，过滤杂质。\n",
    "    # 3) 通过根据问题得到的主题来归纳文本，防止盲目归纳文本出现幻觉。\n",
    "# ok手动实现多个链条。注意不是简单的LLMChain提问，而是每次都和db交互的向量数据库方案链条\n",
    "chainone = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=db.as_retriever())\n",
    "query=\"根据文本的提问,概括出文本有哪些主题?直接回复结果\"\n",
    "resultone = chainone({\"query\": query})\n",
    "theme=resultone[\"result\"]\n",
    "print(\"提炼的主题: \",theme) #回答result[\"result\"]\n",
    "print(\"提炼的主题:  文本的主题包括：振镜水平的高低判断，镜片定制，材料来源，大功率切割硬件套与激光振镜的比较，打标软件的技术难度，以及企业技术优势和目的。\")\n",
    " # ---------------------------------------------------- #\n",
    "chaintwo = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=db.as_retriever())\n",
    "query=f\"“{theme}”,\\n\\n根据上面的主题,请用几句话描述和精简文本的内容？\"\n",
    "resulttwo = chaintwo({\"query\": query})\n",
    "# print(\"问题: \",resulttwo[\"query\"])\n",
    "print(\"总结的结果: \",resulttwo[\"result\"])\n",
    "print(\"总结的结果:  文本主要讨论了振镜水平判断、镜片定制、材料来源、大功率切割硬件套与激光振镜的比较、打标软件的技术难度以及企业技术优势和目的。其中，振镜水平判断的方法主要是价格。镜片定制主要是因为影响温漂等因素。材料来源方面，有些是外采的，有些则需自行制作。大功率切割硬件套与激光振镜的比较主要在于控制部分的技术难度。打标软件的技术难度在于解决工艺问题积累。企业技术优势和目的是在精密加工高端领域与振镜厂商合作定制控制卡等。\")\n",
    " \n",
    "\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
